# Paribu WebSocket Stream API v2 â€” Product Requirements Document

| | |
|---|---|
| **DokÃ¼man** | PRD-2026-003 |
| **ÃœrÃ¼n** | WebSocket Stream API v2 (`stream.paribu.com/v2`) |
| **Sahip** | Platform Engineering |
| **Durum** | Review â†’ Approved |
| **Tarih** | 2026-02-23 |
| **Versiyon** | v2.0 (Review Integrated) |

---

## Ä°Ã§indekiler

1. [Executive Summary](#1-executive-summary)
2. [Problem Statement & Goals](#2-problem-statement--goals)
3. [Non-Goals / Out of Scope](#3-non-goals--out-of-scope)
4. [User Personas](#4-user-personas)
5. [Functional Requirements](#5-functional-requirements)
6. [Non-Functional Requirements](#6-non-functional-requirements)
7. [Technical Architecture](#7-technical-architecture)
8. [Migration Strategy](#8-migration-strategy)
9. [Security Requirements](#9-security-requirements)
10. [Monitoring & Observability](#10-monitoring--observability)
11. [API Reference](#11-api-reference)
12. [Error Handling & Edge Cases](#12-error-handling--edge-cases)
13. [Rate Limiting & Fair Usage](#13-rate-limiting--fair-usage)
14. [Testing Strategy](#14-testing-strategy)
15. [Rollout Plan](#15-rollout-plan)
16. [Success Metrics & KPIs](#16-success-metrics--kpis)
17. [Open Questions / Risks](#17-open-questions--risks)
18. [Competitive Analysis](#18-competitive-analysis)
19. [Phase 3: gRPC Streaming API (Optional)](#19-phase-3-grpc-streaming-api-optional)
20. [Deployment Ä°zolasyon Validasyonu](#20-deployment-izolasyon-validasyonu)

---

## 1. Executive Summary

Paribu, TÃ¼rkiye'nin en bÃ¼yÃ¼k kripto varlÄ±k borsalarÄ±ndan biri olarak v6 API lansmanÄ±na hazÄ±rlanÄ±yor. Mevcut WebSocket altyapÄ±sÄ± (`ws-hub`) basit bir pub/sub relay olarak tasarlanmÄ±ÅŸ olup, Market Maker (MM) ve profesyonel trader'larÄ±n ihtiyaÃ§ duyduÄŸu enterprise-grade Ã¶zellikleri karÅŸÄ±lamamaktadÄ±r.

Bu PRD, `stream.paribu.com/v2` endpoint'i altÄ±nda sunulacak yeni nesil WebSocket Stream API'yi tanÄ±mlar. Hedef: **sequence number'lÄ±, snapshot destekli, reconnection-aware, rate-limited** bir real-time data akÄ±ÅŸÄ± protokolÃ¼ â€” Binance, Bybit ve OKX ile rekabet edebilir kalitede.

**Temel Ã§Ä±ktÄ±lar:**
- 6 public + 6 private channel, standartlaÅŸtÄ±rÄ±lmÄ±ÅŸ `channel@market` format
- Per-channel sequence number ile gap detection
- Orderbook snapshot on subscribe
- Client-initiated ping/pong
- Per-connection rate limiting ve fair usage policy
- Graceful disconnect ile retry-after bilgisi
- `lastSeq` tabanlÄ± reconnection protokolÃ¼
- KrakenD Ã¼zerinden JWT authentication (authorize frame gereksiz)

**Tahmini etki:** MM onboarding sÃ¼resini ~2 haftadan ~2 gÃ¼ne dÃ¼ÅŸÃ¼rmek, orderbook desync vakalarÄ±nÄ± sÄ±fÄ±ra indirmek, 3rd-party integratÃ¶r deneyimini global standarda taÅŸÄ±mak.

---

## 2. Problem Statement & Goals

### 2.1 Problem

Mevcut `ws-hub` altyapÄ±sÄ±, 2022'de internal frontend ihtiyaÃ§larÄ± iÃ§in yazÄ±lmÄ±ÅŸtÄ±r. Market Maker onboarding sÃ¼recinde aÅŸaÄŸÄ±daki kritik eksikler tespit edilmiÅŸtir:

| # | Eksik | Etki |
|---|---|---|
| 1 | Sequence number yok | MM'ler orderbook state'in tutarlÄ± olduÄŸunu doÄŸrulayamÄ±yor. Desync â†’ yanlÄ±ÅŸ fiyatlama â†’ zarar |
| 2 | Snapshot on subscribe yok | Her reconnect'te REST API'den orderbook Ã§ekmek gerekiyor â†’ race condition, ek latency |
| 3 | Client-initiated ping/pong yok | Client tarafÄ±nda connection health monitoring yapÄ±lamÄ±yor |
| 4 | Per-connection rate limit yok | Abuse senaryolarÄ±nda tÃ¼m kullanÄ±cÄ±lar etkileniyor |
| 5 | Reconnection protocol yok | Her disconnect'te full resubscribe + REST snapshot â†’ veri kaybÄ± penceresi |
| 6 | Slow-client backpressure yok | YavaÅŸ client'lar `default:` drop ile mesaj kaybediyor, bilgilendirilmiyor |
| 7 | API key auth yok (WS) | MM'ler bot'larÄ± iÃ§in JWT refresh yÃ¶netimi yapmak zorunda |
| 8 | Graceful disconnect yok | Maintenance sÄ±rasÄ±nda client'lar neden koptuÄŸunu bilmiyor |
| 9 | Standardize payload yok | v1 channel isimleri (`ticker-extended`, `latest-matches`) endÃ¼stri standardÄ±ndan sapÄ±yor |
| 10 | Per-user metrics yok | MM sorun bildirdiÄŸinde debug yapÄ±lamÄ±yor |

### 2.2 Goals

| # | Hedef | Metrik |
|---|---|---|
| G1 | MM'lerin orderbook'u hiÃ§ desync olmadan takip edebilmesi | Desync event = 0 (sequence gap + snapshot ile) |
| G2 | Reconnection sonrasÄ± veri kaybÄ± sÃ¼resi < 100ms | lastSeq resume baÅŸarÄ± oranÄ± > 99% |
| G3 | End-to-end message latency (Kafka â†’ client) < 10ms p99 | Prometheus histogram |
| G4 | MM onboarding sÃ¼resi < 2 gÃ¼n | SDK + dokÃ¼mantasyon ile |
| G5 | 10.000 eÅŸzamanlÄ± connection / pod | Load test ile doÄŸrulanmÄ±ÅŸ |
| G6 | Binance/Bybit/OKX ile feature parity | Competitive analysis checklist |
| G7 | v1 â†’ v2 migration sÄ±rasÄ±nda zero downtime | Blue-green deployment |

### 2.3 Success Criteria

- Ä°lk 3 MM'nin v2'ye geÃ§iÅŸi (launch + 30 gÃ¼n)
- Public stream kullanÄ±cÄ±larÄ±nÄ±n %80'inin v2'ye geÃ§iÅŸi (launch + 90 gÃ¼n)
- v1 deprecation (launch + 180 gÃ¼n)

---

## 3. Non-Goals / Out of Scope

| # | Kapsam DÄ±ÅŸÄ± | Neden |
|---|---|---|
| 1 | WebSocket Ã¼zerinden order placement | REST API (`order-api`) bu iÅŸlevi karÅŸÄ±lÄ±yor. WS order execution ayrÄ± bir PRD gerektirir |
| 2 | FIX protocol desteÄŸi | Åu an iÃ§in MM talebi yok. Ä°leride deÄŸerlendirilebilir |
| 3 | Binary protocol (protobuf/flatbuffers) | JSON yeterli throughput saÄŸlÄ±yor. Ä°htiyaÃ§ halinde v3'te deÄŸerlendirilecek |
| 4 | gRPC streaming API | WebSocket yeterli. gRPC internal servisler arasÄ± kalacak |
| 5 | Futures/margin channel'larÄ± (positions hariÃ§) | Futures Ã¼rÃ¼nÃ¼ henÃ¼z production'da deÄŸil. `positions` channel placeholder olarak tanÄ±mlanacak |
| 6 | Multi-region failover | Tek region (Ä°stanbul) ile baÅŸlanacak. DR ayrÄ± proje |
| 7 | Custom aggregation (VWAP, TWAP stream) | MM'ler bunu kendi taraflarÄ±nda hesaplÄ±yor |
| 8 | Historical data replay | AyrÄ± data service olarak planlanabilir |

---

## 4. User Personas

### 4.1 Market Maker (MM) â€” "Quantitative Ahmet"

| | |
|---|---|
| **Profil** | Algoritmik trading firmasÄ±, 5-20 market'te sÃ¼rekli likidite saÄŸlÄ±yor |
| **BaÄŸlantÄ±** | 5-10 eÅŸzamanlÄ± WS connection, her biri 20-50 channel |
| **Kritik ihtiyaÃ§** | Orderbook consistency (sequence + snapshot), ultra-low latency (<5ms), fills/orders real-time |
| **Hassasiyet** | Tek bir kayÄ±p mesaj = yanlÄ±ÅŸ hedge = potansiyel kayÄ±p |
| **Auth** | API key (bot'lar) + JWT (dashboard). 7/24 uptime beklentisi |
| **Reconnection** | Otomatik, lastSeq tabanlÄ±, <500ms recovery |
| **Mevcut sorun** | ws-hub'da sequence yok â†’ her 30dk'da REST snapshot ile senkronizasyon â†’ race condition |

### 4.2 Retail Trader â€” "GÃ¼nlÃ¼k Ä°ÅŸlemci Elif"

| | |
|---|---|
| **Profil** | Mobil/web app Ã¼zerinden 1-5 market takip ediyor |
| **BaÄŸlantÄ±** | 1 WS connection, 3-10 channel |
| **Kritik ihtiyaÃ§** | GÃ¼venilir ticker + orderbook, basit baÄŸlantÄ± |
| **Auth** | JWT (app Ã¼zerinden) |
| **Reconnection** | App tarafÄ±nda otomatik, full resubscribe kabul edilebilir |
| **Mevcut sorun** | v1 stream sadece public â†’ private data iÃ§in ayrÄ± connection gerekiyor |

### 4.3 3rd Party IntegratÃ¶r â€” "TradingView / CoinGecko / AggregatÃ¶r"

| | |
|---|---|
| **Profil** | Fiyat feed, orderbook depth, trade verisi Ã§eken dÄ±ÅŸ servisler |
| **BaÄŸlantÄ±** | 1-3 WS connection, sadece public channel |
| **Kritik ihtiyaÃ§** | Standart format (kolay parse), yÃ¼ksek uptime, iyi dokÃ¼mantasyon |
| **Auth** | Yok (public) veya API key (rate limit artÄ±ÅŸÄ± iÃ§in) |
| **Reconnection** | Basit resubscribe yeterli |
| **Mevcut sorun** | v1 channel format non-standard â†’ custom parser gerekiyor |

---

## 5. Functional Requirements

### 5.1 Connection Lifecycle

#### 5.1.1 BaÄŸlantÄ± Kurulumu

```
Client                          KrakenD                         ws-hub v2
  |                                |                                |
  |-- WSS UPGRADE /v2 ----------->|                                |
  |   Headers:                     |                                |
  |   Authorization: Bearer <JWT>  |-- JWT validate --------------->|
  |   (veya query: ?token=<JWT>)   |                                |
  |                                |-- WS UPGRADE (user_id header)->|
  |                                |                                |
  |<-- 101 Switching Protocols ----|<-------------------------------|
  |                                |                                |
  |<-- {"event":"connected","ts":..,"connId":"c_abc123"} ----------|
```

**Kurallar:**
- Endpoint: `wss://stream.paribu.com/v2`
- Public baÄŸlantÄ±: Header/token olmadan baÄŸlanabilir
- Private baÄŸlantÄ±: `Authorization: Bearer <JWT>` header veya `?token=<JWT>` query param
- KrakenD JWT validation yapar, `X-User-Id` header'Ä± ile ws-hub'a iletir
- BaÄŸlantÄ± sonrasÄ± `connected` event gÃ¶nderilir (connection ID dahil)
- **Connection ID (`connId`) Format:** `c_{pod_id}_{timestamp_ns}_{random_6char}`
  - Ã–rnek: `c_pod3_1740000000123456_a7f9e2`
  - Pod ID: Hangi pod'da oluÅŸtuÄŸu (debug iÃ§in)
  - Timestamp: OluÅŸturulma zamanÄ± (log correlation)
  - Random suffix: Multi-pod collision prevention
  
> ğŸ“‹ **Review Notu:** connId format belirsiz bulunmuÅŸ (M3). **Ã‡Ã¶zÃ¼m:** Multi-pod unique ID iÃ§in pod_id + timestamp_ns + random suffix kullanÄ±lacaktÄ±r.

#### 5.1.2 Subscribe / Unsubscribe

**Subscribe:**
```json
{
  "method": "subscribe",
  "params": ["ticker@btc_tl", "orderbook.20@eth_tl", "orders@btc_tl"],
  "id": 1
}
```

**Subscribe Response (Success):**
```json
{
  "id": 1,
  "code": 0,
  "msg": "ok",
  "data": {
    "subscribed": ["ticker@btc_tl", "orderbook.20@eth_tl", "orders@btc_tl"]
  }
}
```

**Subscribe Response (Partial â€” bazÄ± channel'lar hatalÄ±):**
```json
{
  "id": 1,
  "code": 0,
  "msg": "partial",
  "data": {
    "subscribed": ["ticker@btc_tl", "orderbook.20@eth_tl"],
    "errors": [
      {"channel": "orders@btc_tl", "code": 40100, "msg": "auth required"}
    ]
  }
}
```

**Unsubscribe:**
```json
{
  "method": "unsubscribe",
  "params": ["ticker@btc_tl"],
  "id": 2
}
```

**Kurallar:**
- Tek request'te birden fazla channel subscribe edilebilir
- Partial success desteklenir (bazÄ±larÄ± baÅŸarÄ±lÄ±, bazÄ±larÄ± hatalÄ±)
- Private channel'lara subscribe iÃ§in JWT gerekir
- Zaten subscribe olunan channel tekrar subscribe edilirse: idempotent (hata dÃ¶nmez)
- Unsubscribe edilmemiÅŸ channel'a unsubscribe: idempotent

#### 5.1.3 Ping / Pong (Client-Initiated)

```json
// Client â†’ Server
{"method": "ping", "id": 3}

// Server â†’ Client
{"method": "pong", "id": 3, "ts": 1740000000000}
```

**Kurallar:**
- Client istediÄŸi zaman ping gÃ¶nderebilir
- Server `pong` ile cevaplar (server timestamp dahil)
- Server ayrÄ±ca WebSocket protocol-level ping frame gÃ¶nderir (30s interval)
- Client 60 saniye iÃ§inde hiÃ§bir mesaj (data veya pong) almazsa baÄŸlantÄ±yÄ± dead kabul etmelidir
- Server, client'tan 60 saniye boyunca hiÃ§bir mesaj (subscribe, ping, vb.) almazsa baÄŸlantÄ±yÄ± kapatÄ±r

#### 5.1.4 Graceful Disconnect

Server maintenance veya rate limit aÅŸÄ±mÄ± durumunda:

```json
// Server â†’ Client (WS close frame'den Ã¶nce)
{
  "event": "disconnecting",
  "code": 4029,
  "msg": "rate limit exceeded",
  "retryAfterMs": 5000
}
```

ArdÄ±ndan WebSocket close frame: `code=4029, reason="rate limit exceeded"`

**Close Codes:**

| WS Close Code | AnlamÄ± | Client DavranÄ±ÅŸÄ± |
|---|---|---|
| 1000 | Normal close | Reconnect gerekmez |
| 1001 | Server going away (deploy) | Hemen reconnect |
| 4000 | Internal error | Exponential backoff ile reconnect |
| 4001 | Invalid message format | Reconnect gereksiz, client hatalÄ± |
| 4003 | Auth failed | Token yenile, sonra reconnect |
| 4029 | Rate limit exceeded | `retryAfterMs` kadar bekle |
| 4030 | Too many connections | Mevcut connection'larÄ± kapat veya bekle |
| 4040 | Maintenance | `retryAfterMs` kadar bekle |

### 5.2 Channel Specifications

#### 5.2.1 Public Channels

##### `ticker@{market}`

24 saatlik market istatistikleri. Her deÄŸiÅŸiklikte push (throttled: max 1/s per market).

```json
{
  "ch": "ticker@btc_tl",
  "ts": 1740000000000,
  "seq": 48291,
  "data": {
    "last": "2850000.00",
    "bid": "2849500.00",
    "ask": "2850100.00",
    "high": "2900000.00",
    "low": "2800000.00",
    "vol": "142.38",
    "quoteVol": "405000000.00",
    "change": "1.82",
    "openPrice": "2799000.00",
    "closeTime": 1740086400000,
    "tradeCount": 12847
  }
}
```

| Alan | Tip | AÃ§Ä±klama |
|---|---|---|
| `last` | string (decimal) | Son iÅŸlem fiyatÄ± |
| `bid` | string (decimal) | En iyi alÄ±ÅŸ fiyatÄ± |
| `ask` | string (decimal) | En iyi satÄ±ÅŸ fiyatÄ± |
| `high` | string (decimal) | 24s en yÃ¼ksek |
| `low` | string (decimal) | 24s en dÃ¼ÅŸÃ¼k |
| `vol` | string (decimal) | 24s base hacim |
| `quoteVol` | string (decimal) | 24s quote hacim |
| `change` | string (decimal) | 24s deÄŸiÅŸim (%) |
| `openPrice` | string (decimal) | 24s aÃ§Ä±lÄ±ÅŸ fiyatÄ± |
| `closeTime` | integer | 24s pencere bitiÅŸ zamanÄ± (ms) |
| `tradeCount` | integer | 24s iÅŸlem sayÄ±sÄ± |

##### `orderbook@{market}` (Incremental / Delta)

Orderbook deÄŸiÅŸiklikleri. Her match engine update'inde push.

**Ä°lk mesaj (subscribe sonrasÄ±): Snapshot**
```json
{
  "ch": "orderbook@btc_tl",
  "ts": 1740000000000,
  "seq": 10000,
  "type": "snapshot",
  "data": {
    "bids": [
      ["2849500.00", "1.20"],
      ["2849000.00", "3.50"],
      ["2848500.00", "0.80"]
    ],
    "asks": [
      ["2850100.00", "0.85"],
      ["2850500.00", "2.30"],
      ["2851000.00", "1.10"]
    ]
  }
}
```

**Sonraki mesajlar: Delta**
```json
{
  "ch": "orderbook@btc_tl",
  "ts": 1740000000050,
  "seq": 10001,
  "type": "delta",
  "data": {
    "bids": [
      ["2849500.00", "0.00"],
      ["2849700.00", "2.10"]
    ],
    "asks": [
      ["2850050.00", "1.50"]
    ]
  }
}
```

**Kurallar:**
- `amount = "0.00"` â†’ ilgili fiyat seviyesi silinmiÅŸ demektir
- Snapshot full orderbook iÃ§erir (tÃ¼m seviyeler)
- Delta sadece deÄŸiÅŸen seviyeleri iÃ§erir
- Client `seq` takip etmelidir. Gap tespit edilirse â†’ unsubscribe + resubscribe (yeni snapshot alÄ±r)
- Snapshot her zaman `seq` numarasÄ± ile gelir; delta'lar bu `seq`'den devam eder

##### `orderbook.{depth}@{market}` (Snapshot â€” Fixed Depth)

Sabit derinlikte orderbook snapshot'Ä±. Her deÄŸiÅŸiklikte full snapshot push.

**depth**: `5`, `10`, `20`

```json
{
  "ch": "orderbook.20@btc_tl",
  "ts": 1740000000000,
  "seq": 10001,
  "type": "snapshot",
  "data": {
    "bids": [
      ["2849500.00", "1.20"],
      ["2849000.00", "3.50"]
    ],
    "asks": [
      ["2850100.00", "0.85"],
      ["2850500.00", "2.30"]
    ],
    "lastUpdateId": 9928341
  }
}
```

**Kurallar:**
- Her mesaj full snapshot (delta yok)
- Push frequency: max 100ms interval (throttled)
- `lastUpdateId`: Match engine'den gelen global sequence
- MM'ler genellikle `orderbook@{market}` (delta) tercih eder; `orderbook.{depth}` retail / integratÃ¶r iÃ§in

##### `trades@{market}`

Real-time public iÅŸlemler.

```json
{
  "ch": "trades@btc_tl",
  "ts": 1740000000000,
  "seq": 77210,
  "data": [
    {
      "tradeId": "t_8829101",
      "price": "2849800.00",
      "amount": "0.35",
      "side": "buy",
      "ts": 1740000000000
    }
  ]
}
```

**Kurallar:**
- `data` array olarak gelir (aynÄ± anda birden fazla trade olabilir â€” batch)
- `side`: Taker'Ä±n tarafÄ± (`buy` = taker alÄ±cÄ±, `sell` = taker satÄ±cÄ±)
- Subscribe sonrasÄ± son 50 trade snapshot olarak gÃ¶nderilir

##### `kline.{interval}@{market}` (Candlestick)

```json
{
  "ch": "kline.1m@btc_tl",
  "ts": 1740000000000,
  "seq": 3201,
  "data": {
    "openTime": 1739999940000,
    "closeTime": 1740000000000,
    "open": "2848000.00",
    "high": "2850200.00",
    "low": "2847500.00",
    "close": "2849800.00",
    "vol": "12.38",
    "quoteVol": "35250000.00",
    "tradeCount": 247,
    "closed": false
  }
}
```

**Intervals:** `1m`, `3m`, `5m`, `15m`, `30m`, `1h`, `2h`, `4h`, `6h`, `12h`, `1d`, `1w`

**Kurallar:**
- Aktif candle her trade'de gÃ¼ncellenir (max 1 push/s throttle)
- `closed: true` â†’ candle kapanmÄ±ÅŸ, artÄ±k deÄŸiÅŸmeyecek
- Subscribe sonrasÄ± aktif (kapanmamÄ±ÅŸ) candle snapshot olarak gÃ¶nderilir

##### `bbo@{market}` (Best Bid/Offer)

```json
{
  "ch": "bbo@btc_tl",
  "ts": 1740000000000,
  "seq": 92001,
  "data": {
    "bid": "2849500.00",
    "bidQty": "1.20",
    "ask": "2850100.00",
    "askQty": "0.85"
  }
}
```

**Kurallar:**
- Her best bid/ask deÄŸiÅŸikliÄŸinde push
- Minimum overhead â€” MM'ler spread monitoring iÃ§in kullanÄ±r
- Throttle yok (her deÄŸiÅŸiklik anÄ±nda)

#### 5.2.2 Private Channels

Private channel'lar JWT authentication gerektirir. KrakenD JWT validate eder ve `X-User-Id` header'Ä± ile ws-hub'a iletir.

##### `orders@{market}` / `orders`

```json
{
  "ch": "orders@btc_tl",
  "ts": 1740000000000,
  "seq": 5001,
  "data": {
    "orderId": "ord_abc123",
    "clientOrderId": "my_order_001",
    "market": "btc_tl",
    "status": "partially_filled",
    "type": "limit",
    "side": "buy",
    "price": "2849000.00",
    "amount": "1.00",
    "filled": "0.35",
    "remaining": "0.65",
    "avgPrice": "2849000.00",
    "fee": "0.99",
    "feeAsset": "TRY",
    "createdAt": 1740000000000,
    "updatedAt": 1740000000050
  }
}
```

**Status deÄŸerleri:** `new`, `partially_filled`, `filled`, `cancelled`, `expired`, `rejected`

**Kurallar:**
- `orders@{market}`: Sadece belirli market
- `orders`: TÃ¼m marketler (MM'ler iÃ§in)
- Her iki channel'a aynÄ± anda subscribe olunabilir (duplicate mesaj gelmez â€” specific market varsa sadece o channel'dan)
- `clientOrderId`: REST API'den order verirken belirtilen client tarafÄ± ID
- Subscribe sonrasÄ± aÃ§Ä±k order'larÄ±n snapshot'Ä± gÃ¶nderilir

##### `fills@{market}` / `fills`

```json
{
  "ch": "fills@btc_tl",
  "ts": 1740000000000,
  "seq": 3001,
  "data": {
    "tradeId": "t_789012",
    "orderId": "ord_abc123",
    "clientOrderId": "my_order_001",
    "market": "btc_tl",
    "side": "buy",
    "price": "2849000.00",
    "amount": "0.35",
    "fee": "0.99",
    "feeAsset": "TRY",
    "isMaker": true,
    "ts": 1740000000000
  }
}
```

**Kurallar:**
- `fills@{market}`: Sadece belirli market
- `fills`: TÃ¼m marketler
- `isMaker`: `true` = maker fee, `false` = taker fee
- Subscribe sonrasÄ± snapshot gÃ¶nderilmez (fills geÃ§miÅŸe dÃ¶nÃ¼k deÄŸil)

##### `balances`

```json
{
  "ch": "balances",
  "ts": 1740000000000,
  "seq": 200,
  "data": {
    "asset": "BTC",
    "available": "1.5000",
    "locked": "0.6500",
    "total": "2.1500"
  }
}
```

**Kurallar:**
- Her bakiye deÄŸiÅŸikliÄŸinde (order placement, fill, deposit, withdrawal) push
- Subscribe sonrasÄ± tÃ¼m asset'lerin mevcut bakiyesi snapshot olarak gÃ¶nderilir
- `locked`: AÃ§Ä±k order'larda kilitli miktar
- `total` = `available` + `locked`

##### `positions`

> **Not:** Bu channel ÅŸu an placeholder'dÄ±r. Futures Ã¼rÃ¼nÃ¼ launch edildiÄŸinde aktifleÅŸtirilecektir.

```json
{
  "ch": "positions",
  "ts": 1740000000000,
  "seq": 50,
  "data": {
    "market": "btc_usdt_perp",
    "side": "long",
    "size": "1.00",
    "entryPrice": "65000.00",
    "markPrice": "65500.00",
    "unrealizedPnl": "500.00",
    "leverage": "10",
    "liquidationPrice": "59000.00"
  }
}
```

### 5.3 Protocol Frame Format

TÃ¼m mesajlar JSON text frame olarak gÃ¶nderilir/alÄ±nÄ±r.

#### 5.3.1 Client â†’ Server MesajlarÄ±

| Method | AÃ§Ä±klama | Rate Limit |
|---|---|---|
| `subscribe` | Channel'lara subscribe | 10 req/s |
| `unsubscribe` | Channel'lardan unsubscribe | 10 req/s |
| `ping` | Connection health check | 5 req/s |

**Format:**
```json
{
  "method": "subscribe|unsubscribe|ping",
  "params": ["channel1", "channel2"],
  "id": <integer>
}
```

- `id`: Client tarafÄ±ndan atanan request ID. Server response'ta aynÄ± `id`'yi dÃ¶ner. 1-2^31 arasÄ± integer.
- `params`: `subscribe` ve `unsubscribe` iÃ§in zorunlu. `ping` iÃ§in opsiyonel (gÃ¶nderilmezse yoksayÄ±lÄ±r).

#### 5.3.2 Server â†’ Client MesajlarÄ±

**Data frame:**
```json
{
  "ch": "<channel>",
  "ts": <server_timestamp_ms>,
  "seq": <sequence_number>,
  "type": "snapshot|delta",   // sadece orderbook channel'larÄ±nda
  "data": { ... }
}
```

**Response frame:**
```json
{
  "id": <request_id>,
  "code": <error_code>,
  "msg": "<message>",
  "data": { ... }           // opsiyonel
}
```

**Event frame:**
```json
{
  "event": "<event_type>",
  "ts": <server_timestamp_ms>,
  ...
}
```

Event types: `connected`, `disconnecting`

### 5.4 Sequence Number Semantics

- Her `(userId, channel)` Ã§ifti iÃ§in monoton artan integer
- Public channel'larda: global sequence (tÃ¼m subscriber'lar aynÄ± seq gÃ¶rÃ¼r)
- Private channel'larda: user-specific sequence
- Gap detection: Client `seq` N aldÄ±ysa, bir sonraki `seq` N+1 olmalÄ±dÄ±r
- Gap tespit edildiÄŸinde client yapmasÄ± gerekenler:
  1. `unsubscribe` â†’ `subscribe` (yeni snapshot alÄ±r)
  2. Veya connection'Ä± kapatÄ±p reconnect

**Sequence overflow:** 2^53 (JavaScript safe integer). Pratikte overflow olmaz (~285 milyon yÄ±l @ 1M msg/s).

### 5.5 Snapshot on Subscribe

AÅŸaÄŸÄ±daki channel'lar subscribe sonrasÄ± otomatik snapshot gÃ¶nderir:

| Channel | Snapshot Ä°Ã§eriÄŸi |
|---|---|
| `orderbook@{market}` | Full orderbook (tÃ¼m seviyeler) |
| `orderbook.{depth}@{market}` | Top N seviye |
| `trades@{market}` | Son 50 trade |
| `kline.{interval}@{market}` | Aktif (kapanmamÄ±ÅŸ) candle |
| `orders@{market}` / `orders` | AÃ§Ä±k order'lar |
| `balances` | TÃ¼m asset bakiyeleri |

Snapshot mesajÄ±nÄ±n `type` alanÄ± `"snapshot"` olarak set edilir (orderbook iÃ§in). DiÄŸer channel'larda ilk mesaj olarak gÃ¶nderilir.

**Snapshot Metadata (Multi-pod Consistency Ä°Ã§in):**

```json
{
  "ch": "orderbook@btc_tl",
  "seq": 10000,              // Bu pod'un bu channel'daki sequence
  "snapshotSeq": 10000,      // Snapshot'Ä±n dayandÄ±ÄŸÄ± sequence
  "lastUpdateId": 9928341,   // Match engine global sequence
  "type": "snapshot",
  "data": { ... }
}
```

**Rationale:** Multi-pod scenario'da consumer lag varsa, farklÄ± pod'tan gelen snapshot stale olabilir. Client, gelen delta'larÄ±n `snapshotSeq` ile consistency kontrol edebilir. Gap tespit edilirse â†’ resubscribe trigger.

> ğŸ“‹ **Review Notu:** Multi-pod snapshot staleness risk tespit edilmiÅŸ (M5). **Ã‡Ã¶zÃ¼m:** Snapshot payload'Ä±na `snapshotSeq` ve `lastUpdateId` metadata eklenmiÅŸdir. Client, delta seq validation'Ä±nda bu metadata'yÄ± kullanabilir (optional, client SDK responsibility).

### 5.6 Reconnection Protocol

#### 5.6.1 Basit Reconnection (Retail)

1. BaÄŸlantÄ±yÄ± yeniden kur
2. TÃ¼m channel'lara tekrar subscribe ol
3. Snapshot'larÄ± al, devam et

#### 5.6.2 Fast Resume (MM / Enterprise)

```json
// Reconnect sonrasÄ± subscribe
{
  "method": "subscribe",
  "params": ["orderbook@btc_tl"],
  "id": 1,
  "lastSeq": {
    "orderbook@btc_tl": 10050
  }
}
```

**Server davranÄ±ÅŸÄ±:**
- `lastSeq` varsa ve server buffer'Ä±nda bu seq'den sonraki mesajlar mevcutsa â†’ gap mesajlarÄ± gÃ¶nderilir (snapshot olmadan)
- Buffer'da yoksa (Ã§ok eski veya buffer taÅŸmÄ±ÅŸ) â†’ normal snapshot gÃ¶nderilir
- `lastSeq` yoksa â†’ normal snapshot

**Server buffer:** Son 5 dakikalÄ±k mesajlar per-channel in-memory buffer'da tutulur.

### 5.7 Authentication Flow

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Client  â”‚
                    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
                         â”‚
           WSS UPGRADE /v2
           Authorization: Bearer <JWT>
                         â”‚
                    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
                    â”‚ KrakenD  â”‚
                    â”‚  Gateway â”‚
                    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
                         â”‚
              JWT Validate (RS256)
              Extract: user_id, tier, permissions
                         â”‚
              X-User-Id: "u_12345"
              X-User-Tier: "mm"
                         â”‚
                    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
                    â”‚ ws-hub   â”‚
                    â”‚   v2     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Auth seviyeleri:**

| Seviye | EriÅŸim | NasÄ±l |
|---|---|---|
| Anonymous | Public channel'lar | Header/token olmadan baÄŸlan |
| Authenticated | Public + Private channel'lar | JWT token ile baÄŸlan |
| MM Tier | Public + Private + geniÅŸletilmiÅŸ limitler | JWT + `tier=mm` claim |

**JWT Token Refresh:**
- JWT expiry KrakenD tarafÄ±ndan kontrol edilir
- Token expire olduÄŸunda KrakenD mevcut WS connection'Ä± kapatmaz (connection zaten kurulmuÅŸ)
- Yeni connection iÃ§in yeni token gerekir
- Token lifetime: 24 saat (configurable)

---

## 6. Non-Functional Requirements

### 6.1 Latency

| Metrik | Hedef | Ã–lÃ§Ã¼m NoktasÄ± |
|---|---|---|
| Kafka â†’ Client (e2e) | < 10ms p50, < 25ms p99 | Prometheus histogram |
| Snapshot delivery | < 50ms p99 | Subscribe request'ten ilk data frame'e |
| Ping-pong RTT | < 5ms p99 | Client-measured |
| Subscribe ack | < 10ms p99 | Request'ten response'a |

### 6.2 Throughput

| Metrik | Hedef |
|---|---|
| Messages/sec per pod | 500,000 outbound |
| Connections per pod | 10,000 concurrent |
| Subscribe requests per connection per second | 10 |
| Channels per connection | 200 max |
| Total channels across all connections (per user) | 1,000 max |

### 6.3 Availability

| Metrik | Hedef |
|---|---|
| Uptime SLA | 99.95% (yÄ±llÄ±k ~4.4 saat downtime) |
| Planned maintenance window | < 30 saniye (graceful disconnect + reconnect) |
| Recovery Time Objective (RTO) | < 60 saniye |
| Recovery Point Objective (RPO) | 0 (Kafka replay) |

### 6.4 Scalability

| Metrik | Hedef |
|---|---|
| Horizontal scale | Pod eklenerek linear Ã¶lÃ§eklenme |
| Max concurrent connections (cluster) | 100,000 |
| Max markets | 500 |
| Kafka partition per topic | Market sayÄ±sÄ± ile orantÄ±lÄ± (1:1 veya N:1) |

### 6.5 Compression

- `permessage-deflate` WebSocket extension varsayÄ±lan aktif
- Context takeover: server=yes, client=yes (sliding window 32KB)
- Tahmini compression ratio: ~70-80% (JSON text iÃ§in)
- Client deflate desteklemiyorsa: uncompressed fallback

### 6.6 Message Size

| | Limit |
|---|---|
| Max inbound message (client â†’ server) | 4 KB |
| Max outbound message (server â†’ client) | 1 MB (orderbook full snapshot) |
| Typical outbound message | 200-500 bytes |

---

## 7. Technical Architecture

### 7.1 High-Level Architecture

```
                                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                   â”‚              Kubernetes Cluster              â”‚
                                   â”‚                                             â”‚
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    WSS     â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
  â”‚  Client   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚   KrakenD     â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚   ws-hub v2      â”‚        â”‚
  â”‚  (MM/Web) â”‚           â”‚   Gateway     â”‚  HTTP   â”‚   (Go, N pods)   â”‚        â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚               â”‚  Upgradeâ”‚                  â”‚        â”‚
                          â”‚  - JWT auth   â”‚         â”‚  - Hub manager   â”‚        â”‚
                          â”‚  - Rate limit â”‚         â”‚  - Seq manager   â”‚        â”‚
                          â”‚  - Compressionâ”‚         â”‚  - Snapshot svc  â”‚        â”‚
                          â”‚  - Bot detect â”‚         â”‚  - Buffer mgr    â”‚        â”‚
                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚  - Metrics       â”‚        â”‚
                                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
                                                             â”‚                  â”‚
                                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
                                              â”‚              â”‚              â”‚   â”‚
                                         â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”â”‚
                                         â”‚  Kafka  â”‚   â”‚  Snapshot â”‚  â”‚ Redis â”‚â”‚
                                         â”‚(Redpandaâ”‚   â”‚  Service  â”‚  â”‚(opt.) â”‚â”‚
                                         â”‚)        â”‚   â”‚  (gRPC)   â”‚  â”‚       â”‚â”‚
                                         â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
                                              â”‚              â”‚                  â”‚
                                         â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”           â”‚
                                         â”‚  Match  â”‚   â”‚  Order    â”‚           â”‚
                                         â”‚  Engine â”‚   â”‚  API      â”‚           â”‚
                                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
                                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 7.2 ws-hub v2 Ä°Ã§ Mimari

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      ws-hub v2 pod                       â”‚
â”‚                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Connection  â”‚    â”‚   Channel    â”‚    â”‚  Kafka     â”‚ â”‚
â”‚  â”‚  Manager     â”‚â—„â”€â”€â–ºâ”‚   Router     â”‚â—„â”€â”€â”€â”‚  Consumer  â”‚ â”‚
â”‚  â”‚             â”‚    â”‚              â”‚    â”‚  Group     â”‚ â”‚
â”‚  â”‚  - Accept   â”‚    â”‚  - Subscribe â”‚    â”‚            â”‚ â”‚
â”‚  â”‚  - Auth     â”‚    â”‚  - Unsubscribeâ”‚   â”‚  Topics:   â”‚ â”‚
â”‚  â”‚  - Rate Lmt â”‚    â”‚  - Dispatch  â”‚    â”‚  - ticker  â”‚ â”‚
â”‚  â”‚  - Ping/Pongâ”‚    â”‚  - Snapshot  â”‚    â”‚  - ob-deltaâ”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  - trades  â”‚ â”‚
â”‚         â”‚                  â”‚            â”‚  - orders  â”‚ â”‚
â”‚         â”‚           â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”    â”‚  - fills   â”‚ â”‚
â”‚         â”‚           â”‚  Sequence    â”‚    â”‚  - balance â”‚ â”‚
â”‚         â”‚           â”‚  Manager     â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚         â”‚           â”‚              â”‚                    â”‚
â”‚         â”‚           â”‚  - Per-ch seqâ”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚         â”‚           â”‚  - Gap bufferâ”‚    â”‚  Snapshot   â”‚ â”‚
â”‚         â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  Cache     â”‚ â”‚
â”‚         â”‚                               â”‚            â”‚ â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”                        â”‚  - OB full â”‚ â”‚
â”‚  â”‚  Write      â”‚                        â”‚  - Trades  â”‚ â”‚
â”‚  â”‚  Scheduler  â”‚                        â”‚  - Kline   â”‚ â”‚
â”‚  â”‚             â”‚                        â”‚  - Orders  â”‚ â”‚
â”‚  â”‚  - Per-conn â”‚                        â”‚  - Balance â”‚ â”‚
â”‚  â”‚    queue    â”‚                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚  â”‚  - Backpres.â”‚                                       â”‚
â”‚  â”‚  - Batch    â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  Metrics   â”‚                     â”‚
â”‚                     â”‚  Exporter  â”‚                     â”‚
â”‚                     â”‚  (Prom)    â”‚                     â”‚
â”‚                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 7.3 BileÅŸen DetaylarÄ±

#### 7.3.1 Connection Manager

- **Sorumluluk:** WebSocket connection lifecycle (accept, close, auth context)
- **Veri yapÄ±sÄ±:** `map[connId]*Connection`
- **Connection struct:**
  ```go
  type Connection struct {
      ID          string
      UserID      string          // "" for anonymous
      Tier        string          // "anonymous", "retail", "mm"
      Conn        *websocket.Conn
      Channels    map[string]bool // subscribed channels
      WriteCh     chan []byte     // buffered write channel
      LastActivity time.Time
      CreatedAt   time.Time
      RemoteAddr  string
      ConnMeta    ConnMeta        // rate limit counters, etc.
  }
  ```
- **Per-IP connection tracking:** `map[IP]int` (anonymous baÄŸlantÄ±lar iÃ§in)
- **Per-User connection tracking:** `map[UserID]int` (authenticated baÄŸlantÄ±lar iÃ§in)

#### 7.3.2 Channel Router

- **Sorumluluk:** Channel â†’ Connection mapping, message fan-out
- **Veri yapÄ±sÄ±:** `map[channel]map[connId]*Connection`
- **Fan-out:** Kafka'dan gelen mesaj â†’ channel'a subscribe olan tÃ¼m connection'lara write
- **Optimizasyon:** Mesaj bir kez serialize edilir, tÃ¼m connection'lara aynÄ± `[]byte` gÃ¶nderilir (zero-copy fan-out)

#### 7.3.3 Sequence Manager

- **Sorumluluk:** Per-channel monoton artan sequence number Ã¼retimi
- **Public channels:** Kafka partition offset doÄŸrudan sequence olarak kullanÄ±lÄ±r.
  - âœ… Pod-independent, persistent, overflow riski yok (64-bit Kafka offset)
  - âœ… Multi-pod consistency guarantee
  - **Karar (Q2):** Kafka offset = sequence
  
- **Private channels:** Per-user pod-local atomic counter. User'Ä±n baÄŸlÄ± olduÄŸu pod'da tutulur.
  - Pod restart'ta reset â†’ client'a full snapshot gÃ¶nderilir (fallback mekanizmasÄ±)
  - **Karar (Q2):** Pod-local counter, reconnect farklÄ± pod'a dÃ¼ÅŸerse snapshot fallback
  
- **Gap buffer:** Son 5 dakikalÄ±k mesajlar per-channel ring buffer'da tutulur (reconnection resume iÃ§in)
  - Buffer boyutu: **Per-channel adaptive policy:**
    - Orderbook (@): 100K mesaj
    - Trades (@): 50K mesaj
    - Snapshot channels (.{depth}@, ticker@): 1K mesaj
    - Private channels (orders, fills, balances): 1K mesaj
  - Memory tahmini revize: ~200MB per pod (popular 50 market Ã— optimal buffer size)

> ğŸ“‹ **Review Notu:** Sequence overflow ve pod restart'ta sequence reset riski tespit edilmiÅŸ. **Ã‡Ã¶zÃ¼m:** Public channel'da Kafka offset kullan â†’ persistent ve pod-agnostic. Private channel'da pod-local counter acceptable (pod restart â†’ full snapshot fallback). Gap buffer memory optimization: per-channel size policy tanÄ±mlanmÄ±ÅŸtÄ±r (review'da M8 detaylandÄ±rÄ±lmÄ±ÅŸtÄ±r).

#### 7.3.4 Snapshot Cache

- **Sorumluluk:** Subscribe sonrasÄ± ilk snapshot'Ä± hÄ±zlÄ± gÃ¶ndermek
- **Orderbook snapshot:** In-memory orderbook reconstruction yaklaÅŸÄ±mÄ±
  - ws-hub v2, orderbook delta'larÄ±nÄ± consume ederek kendi in-memory orderbook kopyasÄ±nÄ± tutar
  - Subscribe geldiÄŸinde bu in-memory copy'dan snapshot Ã¼retilir
  - âœ… Match engine'e ek yÃ¼k bindirmez
  - âœ… Snapshot latency ~0ms (memory read)
  - âœ… Consistency: Delta sequence ile orderbook state her zaman sync
  - âš ï¸ Per-market OB boyutu: Max 10K level (daha derin istemler reject edilir)
  - âš ï¸ Memory monitoring: OB cache >1.5GB â†’ alarm, LRU eviction starts

- **Trades snapshot:** ws-hub in-memory buffer (son 50 trade per-market)
  - Subscribe sonrasÄ± buffer'dan serve edilir
  - Redis dependency ortadan kaldÄ±rÄ±lmÄ±ÅŸ
  
- **Kline snapshot:** Kafka'dan consume edilir, in-memory cache'lenir (aktif candle)
  
- **Orders/Balances snapshot:** order-api / wallet servisine gRPC call
  - **Karar (Q6):** Capacity test gerekli (v2 snapshot load'Ä± order-api handle edebilir mi)
  - **Action:** Phase 1 beta'da order-api load monitÃ¶r edilmeli

> ğŸ“‹ **Review Notu:** Snapshot service'in v2 yÃ¼kÃ¼ ile ilgili izolasyon riski tespit edilmiÅŸ. **Ã‡Ã¶zÃ¼m:** Orderbook ve trades snapshot'larÄ± ws-hub'da in-memory tuple yapÄ±lmÄ±ÅŸtÄ±r â†’ external dependency yok. Orders/balances: order-api ve wallet servislerine dependency var, capacity test gerekli. Per-market OB boyut limiti ve memory alerting eklenmemiÅŸtir â†’ management policy tanÄ±mlanmÄ±ÅŸtÄ±r.

#### 7.3.5 Write Scheduler (Backpressure)

- **Sorumluluk:** Her connection'a mesaj yazma, slow client yÃ¶netimi
- **Per-connection write channel:** Buffered channel (capacity: 256 mesaj)
- **Backpressure stratejisi:**
  1. Write channel dolu â†’ mesaj drop edilir (eski davranÄ±ÅŸ: `default: drop`)
  2. **Yeni:** Drop counter artÄ±rÄ±lÄ±r. 10 saniyede 100'den fazla drop â†’ client'a warning mesajÄ±:
     ```json
     {"event": "slow_consumer", "dropped": 147, "window": "10s"}
     ```
  3. 60 saniyede 1000'den fazla drop â†’ graceful disconnect (code 4000)
- **Batching:** Write scheduler 1ms window iÃ§inde biriken mesajlarÄ± tek write'a birleÅŸtirir (optional, configurable)

### 7.4 Kafka Topic YapÄ±sÄ±

| Topic | Key | Partitions | Producers | Consumer (ws-hub) |
|---|---|---|---|---|
| `ws.ticker` | market | 64 | Match engine | Broadcast to `ticker@{market}` |
| `ws.orderbook.delta` | market | 64 | Match engine | Broadcast to `orderbook@{market}`, update in-memory OB |
| `ws.trades` | market | 64 | Match engine | Broadcast to `trades@{market}` |
| `ws.kline` | market+interval | 64 | Kline aggregator | Broadcast to `kline.{interval}@{market}` |
| `ws.bbo` | market | 64 | Match engine | Broadcast to `bbo@{market}` |
| `ws.orders` | user_id | 64 | Order API | Route to `orders@{market}` / `orders` |
| `ws.fills` | user_id | 64 | Match engine | Route to `fills@{market}` / `fills` |
| `ws.balances` | user_id | 64 | Wallet | Route to `balances` |

**Consumer group:** `ws-hub-v2`
- Public topic'ler: Her pod tÃ¼m partition'larÄ± consume eder (broadcast pattern)
- Private topic'ler: Partition assignment. User'Ä±n baÄŸlÄ± olduÄŸu pod, o user'Ä±n partition'Ä±nÄ± consume eder.

> ğŸ“‹ **Review Notu:** Partition sayÄ±sÄ± review'da "Market sayÄ±sÄ± ile orantÄ±lÄ±" ifadesi belirsiz bulunmuÅŸ. **Ã–nerilen:** 64 sabit partition (market count'tan baÄŸÄ±msÄ±z) â†’ yeterli parallelism ve manageable complexity. Topic key strategy: market-keyed (public) ve user_id-keyed (private) â†’ sÄ±rasÄ±yla market ve user'a gÃ¶re ordering guarantee.

**Private channel routing problemi:**
User herhangi bir pod'a baÄŸlanabilir, ama private mesajlarÄ± sadece o user'Ä±n partition'Ä±nÄ± consume eden pod alÄ±r.

**Ã‡Ã¶zÃ¼m seÃ§enekleri:**
1. **Internal relay:** Pod-to-pod gRPC stream. Partition owner pod â†’ user'Ä±n baÄŸlÄ± olduÄŸu pod'a forward.
2. **Broadcast private topics:** Her pod tÃ¼m private partition'larÄ± da consume eder, user kendi pod'unda deÄŸilse drop eder. (Basit ama wasteful)
3. **Sticky routing:** KrakenD'de user_id hash ile pod seÃ§imi (consistent hashing). User her zaman aynÄ± pod'a dÃ¼ÅŸer.

**Ã–nerilen:** Opsiyon 3 (Sticky routing via KrakenD). AvantajlarÄ±:
- Basit implementasyon
- Kafka consumer group efficient partition assignment
- User-level state (sequence, subscriptions) tek pod'da
- Dezavantaj: Pod restart'ta user'lar yeni pod'a dÃ¼ÅŸer â†’ reconnect. Graceful disconnect ile mitigate edilir.

### 7.5 Deployment Topology

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Kubernetes                         â”‚
â”‚                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚ KrakenD (3 pod) â”‚   â”‚ ws-hub v2       â”‚          â”‚
â”‚  â”‚                 â”‚â”€â”€â–ºâ”‚ (5-20 pod, HPA) â”‚          â”‚
â”‚  â”‚ - L7 LB        â”‚   â”‚                 â”‚          â”‚
â”‚  â”‚ - JWT           â”‚   â”‚ - Stateful conn â”‚          â”‚
â”‚  â”‚ - Sticky hash   â”‚   â”‚ - Kafka consumerâ”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                                 â”‚                    â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚         â”‚                       â”‚               â”‚    â”‚
â”‚    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”â”‚
â”‚    â”‚ Kafka   â”‚           â”‚ Match Engineâ”‚  â”‚ Redis  â”‚â”‚
â”‚    â”‚(Redpandaâ”‚           â”‚ (per market)â”‚  â”‚(opt.)  â”‚â”‚
â”‚    â”‚ 3 node) â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**HPA (Horizontal Pod Autoscaler):**

```yaml
metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70    # Primary metric
  
  - type: Pods
    pods:
      metric:
        name: ws_connections_active
      target:
        type: AverageValue
        averageValue: 8000        # Secondary

  - type: Pods
    pods:
      metric:
        name: ws_messages_sent_rate
      target:
        type: AverageValue
        averageValue: 400000      # Tertiary (500K msg/s per pod limit)

behavior:
  scaleUp:
    stabilizationWindowSeconds: 120
    policies:
      - type: Percent
        value: 100                # Double pod count max
        periodSeconds: 60
  
  scaleDown:
    stabilizationWindowSeconds: 600  # 10 min
    policies:
      - type: Percent
        value: 50                 # Remove 50% of excess
        periodSeconds: 120
```

**Rationale (Review M4):** Single metric (connection count) yanÄ±ltÄ±cÄ± olabilir (idle client'lar). CPU + connection + throughput kombinasyonu daha reliable. Scale-down yavaÅŸ (churn avoid) ve scale-up hÄ±zlÄ± (traffic spike'a cevap).

> ğŸ“‹ **Review Notu:** HPA metrik seÃ§imi review'da iyileÅŸtirilmiÅŸtir (M4). **Ã‡Ã¶zÃ¼m:** CPU + connection + message throughput metrikleri birleÅŸtirilmiÅŸtir.

---

## 8. Migration Strategy

### 8.1 Timeline

```
Week 0        Week 4        Week 8        Week 12       Week 24
  â”‚             â”‚             â”‚              â”‚              â”‚
  â–¼             â–¼             â–¼              â–¼              â–¼
  PRD           v2 Beta       v2 GA          v1 Deprecation v1 Shutdown
  Approved      (internal)    (public)       Warning        (EOL)
                + MM beta                    Header
```

### 8.2 Phase Details

#### Phase 1: Beta (Week 0-4)
- ws-hub v2 deploy (ayrÄ± pod set, ayrÄ± KrakenD route)
- Internal test: QA + Platform Engineering
- 2-3 beta MM partner ile private test
- v1 ve v2 paralel Ã§alÄ±ÅŸÄ±r, aynÄ± Kafka topic'lerden consume eder
- Endpoint: `stream.paribu.com/v2` (beta flag ile)

#### Phase 2: GA (Week 4-8)
- Public launch: `stream.paribu.com/v2`
- Documentation publish (docs.paribu.com/api/v2/websocket)
- Python & TypeScript SDK release
- v1 hala aktif, deprecation announcement yok

#### Phase 3: Migration (Week 8-12)
- v1 response'larÄ±na `X-Deprecation: 2026-08-01` header eklenir
- v1 kullanÄ±cÄ±larÄ±na email/notification
- docs.paribu.com'da v1 sayfalarÄ±na deprecation banner

#### Phase 4: Sunset (Week 12-24)
- v1 connection limit kademeli dÃ¼ÅŸÃ¼rÃ¼lÃ¼r (100 â†’ 50 â†’ 10 â†’ 0)
- v1 shutdown tarihi: Week 24
- Emergency rollback planÄ±: v1 pod'larÄ± 4 hafta daha cold standby'da tutulur

### 8.3 Backward Compatibility

| v1 Feature | v2 KarÅŸÄ±lÄ±ÄŸÄ± | Notlar |
|---|---|---|
| `ticker-extended` | `ticker@{market}` | FarklÄ± payload format |
| `orderbook` | `orderbook@{market}` | + snapshot + sequence |
| `latest-matches` | `trades@{market}` | Rename |
| `ticker24h` | `ticker@{market}` | Merge edildi |
| `api-orderbook` | `orderbook.{depth}@{market}` | Depth parametreli |
| `config-public` | Out of scope | REST API'den Ã§ekilecek |
| `closed-orders` | `orders@{market}` (status=filled/cancelled) | Unified |
| `open-orders` | `orders@{market}` (status=new/partial) | Unified |
| `assets` | `balances` | Rename |
| `user` | Out of scope (v2'de yok) | Notification ayrÄ± kanal |
| `transactions` | `fills@{market}` | Rename |
| `config-private` | Out of scope | REST API'den Ã§ekilecek |
| Native `/ws` authorize frame | KrakenD JWT | Frame gereksiz |

**Breaking changes:**
- Channel isimleri deÄŸiÅŸiyor (non-backward-compatible)
- Payload format deÄŸiÅŸiyor
- Authorize frame kaldÄ±rÄ±ldÄ±
- Bu nedenle `/v2` ayrÄ± endpoint â€” v1 ile aynÄ± anda Ã§alÄ±ÅŸÄ±r

---

## 9. Security Requirements

### 9.1 Transport Security

| Requirement | Detail |
|---|---|
| TLS | TLS 1.2+ zorunlu. TLS 1.3 tercih edilir |
| Certificate | Let's Encrypt veya enterprise CA (CloudFlare origin cert) |
| Cipher suites | AEAD only (AES-GCM, ChaCha20-Poly1305) |
| HSTS | `Strict-Transport-Security: max-age=31536000` |
| Origin validation | KrakenD `Origin` header kontrolÃ¼ (CORS) |

### 9.2 Authentication & Authorization

| Requirement | Detail |
|---|---|
| JWT algorithm | RS256 (asymmetric) |
| JWT issuer | Paribu Auth Service |
| JWT audience | `stream.paribu.com` |
| JWT expiry | Max 24 saat |
| JWT claims | `sub` (user_id), `tier` (anonymous/retail/mm), `permissions` |
| Private channel auth | JWT zorunlu. JWT yoksa â†’ `40100 Auth required` |
| Token rotation | Client, token expire olmadan yeni baÄŸlantÄ± aÃ§malÄ± |
| Revocation | JWT blacklist (Redis) â€” KrakenD kontrol eder |

### 9.3 Input Validation

| Requirement | Detail |
|---|---|
| Max message size (inbound) | 4 KB. AÅŸÄ±lÄ±rsa â†’ disconnect |
| JSON parse | Strict JSON parse. Invalid JSON â†’ `4001` close |
| Channel name validation | Regex: `^[a-z]+(\.\d+)?@[a-z]+_[a-z]+$` (veya channel listesinde whitelist) |
| Method validation | Sadece `subscribe`, `unsubscribe`, `ping` kabul edilir |
| `id` validation | Integer, 1 â‰¤ id â‰¤ 2^31 |
| `params` validation | Array, max 200 eleman, her biri string max 64 char |

### 9.4 DDoS Protection

| Layer | Protection |
|---|---|
| L3/L4 | CloudFlare / AWS Shield |
| L7 | KrakenD rate limiting + bot detection |
| WS | Per-connection rate limit (subscribe, message) |
| Connection | Max connections per IP / per user |
| Slowloris | KrakenD connection timeout (30s handshake) |

### 9.5 Data Privacy

- Private channel mesajlarÄ± sadece authenticated user'a gÃ¶nderilir
- Connection metadata (IP, user agent) 90 gÃ¼n loglanÄ±r, sonra silinir
- KVKK compliance: User verisi TÃ¼rkiye'de tutulur

---

## 10. Monitoring & Observability

### 10.1 Metrics (Prometheus)

#### Connection Metrics

| Metric | Type | Labels |
|---|---|---|
| `ws_connections_active` | Gauge | `pod`, `tier` |
| `ws_connections_total` | Counter | `pod`, `tier`, `close_code` |
| `ws_connection_duration_seconds` | Histogram | `pod`, `tier` |

#### Message Metrics

| Metric | Type | Labels |
|---|---|---|
| `ws_messages_sent_total` | Counter | `pod`, `channel_type`, `msg_type` |
| `ws_messages_sent_bytes_total` | Counter | `pod`, `channel_type` |
| `ws_messages_received_total` | Counter | `pod`, `method` |
| `ws_message_latency_seconds` | Histogram | `pod`, `channel_type` |
| `ws_messages_dropped_total` | Counter | `pod`, `reason` |

#### Subscription Metrics

| Metric | Type | Labels |
|---|---|---|
| `ws_subscriptions_active` | Gauge | `pod`, `channel` |
| `ws_subscribe_requests_total` | Counter | `pod`, `status` |
| `ws_snapshot_duration_seconds` | Histogram | `pod`, `channel_type` |

#### Kafka Consumer Metrics

| Metric | Type | Labels |
|---|---|---|
| `ws_kafka_consumer_lag` | Gauge | `pod`, `topic`, `partition` |
| `ws_kafka_messages_consumed_total` | Counter | `pod`, `topic` |

#### Rate Limit Metrics

| Metric | Type | Labels |
|---|---|---|
| `ws_rate_limit_exceeded_total` | Counter | `pod`, `limit_type` |
| `ws_slow_consumer_warnings_total` | Counter | `pod` |
| `ws_slow_consumer_disconnects_total` | Counter | `pod` |

### 10.2 Per-User Metrics (MM Debug)

MM'lerin sorun bildirmesi durumunda hÄ±zlÄ± debug iÃ§in:

```
ws_user_messages_sent_total{user_id="u_12345", channel="orderbook@btc_tl"}
ws_user_messages_dropped_total{user_id="u_12345"}
ws_user_connection_count{user_id="u_12345"}
ws_user_latency_p99{user_id="u_12345"}
```

**Not:** Per-user metrics yalnÄ±zca `tier=mm` kullanÄ±cÄ±lar iÃ§in aktif (cardinality kontrolÃ¼).

### 10.3 Logging

| Level | Ä°Ã§erik |
|---|---|
| `INFO` | Connection open/close, subscribe/unsubscribe (connId, userId, channel) |
| `WARN` | Rate limit exceeded, slow consumer warning, sequence gap |
| `ERROR` | Kafka consumer error, snapshot failure, unexpected disconnect |
| `DEBUG` | Her mesaj detayÄ± (sadece debug mode'da, production'da kapalÄ±) |

**Format:** JSON structured logging (OpenTelemetry compatible)

**Correlation:** Her log satÄ±rÄ±nda `connId`, `userId`, `traceId` bulunur.

### 10.4 Alerting

| Alert | Condition | Severity |
|---|---|---|
| High connection count | > 80% capacity per pod | Warning |
| Kafka consumer lag | > 1000 messages for > 30s | Critical |
| Message drop rate | > 1% of total messages | Warning |
| Connection error spike | > 100 errors/min | Critical |
| Snapshot latency | p99 > 200ms | Warning |
| Pod memory | > 80% | Warning |
| Zero connections on pod | = 0 for > 5 min (not during deploy) | Warning |

### 10.5 Distributed Tracing

- OpenTelemetry (OTLP) ile KrakenD â†’ ws-hub â†’ Kafka arasÄ± trace
- Her Kafka mesajÄ±na `traceId` header eklenir
- Grafana Tempo / Jaeger ile visualize

### 10.6 Dashboard

**Grafana Dashboard: "WS Stream v2 Overview"**
- Active connections (by tier, by pod)
- Message throughput (by channel type)
- Kafka consumer lag
- Latency heatmap (e2e)
- Connection churn rate
- Top 10 users by connection count
- Error rate by code

---

## 11. API Reference

### 11.1 Endpoint

```
wss://stream.paribu.com/v2
```

### 11.2 Connection

**Headers:**
| Header | Zorunlu | AÃ§Ä±klama |
|---|---|---|
| `Authorization` | HayÄ±r (public), Evet (private) | `Bearer <JWT>` |

**Query Parameters:**
| Param | Zorunlu | AÃ§Ä±klama |
|---|---|---|
| `token` | HayÄ±r | JWT (header alternatifi, bazÄ± client lib'ler header desteklemez) |

**Ä°lk mesaj (server â†’ client):**
```json
{
  "event": "connected",
  "ts": 1740000000000,
  "connId": "c_abc123def",
  "server": "ws-hub-v2-pod-3"
}
```

### 11.3 Client â†’ Server Methods

#### `subscribe`

```json
{
  "method": "subscribe",
  "params": ["ticker@btc_tl", "orderbook.20@eth_tl"],
  "id": 1
}
```

Opsiyonel (reconnection resume):
```json
{
  "method": "subscribe",
  "params": ["orderbook@btc_tl"],
  "id": 1,
  "lastSeq": {
    "orderbook@btc_tl": 10050
  }
}
```

**Response:**
```json
{
  "id": 1,
  "code": 0,
  "msg": "ok",
  "data": {
    "subscribed": ["ticker@btc_tl", "orderbook.20@eth_tl"]
  }
}
```

#### `unsubscribe`

```json
{
  "method": "unsubscribe",
  "params": ["ticker@btc_tl"],
  "id": 2
}
```

**Response:**
```json
{
  "id": 2,
  "code": 0,
  "msg": "ok",
  "data": {
    "unsubscribed": ["ticker@btc_tl"]
  }
}
```

#### `ping`

```json
{"method": "ping", "id": 3}
```

**Response:**
```json
{"method": "pong", "id": 3, "ts": 1740000000000}
```

### 11.4 Server â†’ Client Data Messages

#### Genel format:

```json
{
  "ch": "<channel_name>",
  "ts": <unix_timestamp_ms>,
  "seq": <sequence_number>,
  "type": "snapshot|delta",
  "data": { ... }
}
```

- `ch`: Channel adÄ±
- `ts`: Server timestamp (millisecond Unix epoch)
- `seq`: Per-channel monoton artan sequence number
- `type`: Sadece orderbook channel'larÄ±nda. DiÄŸerlerinde alan yok.
- `data`: Channel'a Ã¶zgÃ¼ payload

### 11.5 Channel Reference

#### Public Channels

| Channel | Subscribe Format | Snapshot | Throttle | AÃ§Ä±klama |
|---|---|---|---|---|
| `ticker@{market}` | `ticker@btc_tl` | Son ticker | Max 1/s | 24h market stats |
| `orderbook@{market}` | `orderbook@btc_tl` | Full orderbook | Yok | Incremental delta |
| `orderbook.{depth}@{market}` | `orderbook.20@btc_tl` | Snapshot | Max 100ms | depth: 5/10/20 |
| `trades@{market}` | `trades@btc_tl` | Son 50 trade | Yok | Real-time trades |
| `kline.{interval}@{market}` | `kline.1m@btc_tl` | Aktif candle | Max 1/s | Candlestick |
| `bbo@{market}` | `bbo@btc_tl` | Son BBO | Yok | Best bid/offer |

#### Private Channels

| Channel | Subscribe Format | Snapshot | AÃ§Ä±klama |
|---|---|---|---|
| `orders@{market}` | `orders@btc_tl` | AÃ§Ä±k orderlar | Per-market order updates |
| `orders` | `orders` | AÃ§Ä±k orderlar | All-market order updates |
| `fills@{market}` | `fills@btc_tl` | Yok | Per-market fills |
| `fills` | `fills` | Yok | All-market fills |
| `balances` | `balances` | TÃ¼m bakiyeler | Balance changes |
| `positions` | `positions` | AÃ§Ä±k pozisyonlar | Futures (placeholder) |

#### Market Format

`{base}_{quote}` â€” lowercase, underscore separated.

Ã–rnekler: `btc_tl`, `eth_tl`, `btc_usdt`, `sol_usdt`

GeÃ§erli marketler `config-service`'ten alÄ±nÄ±r. GeÃ§ersiz market â†’ `40002 Invalid market`.

### 11.6 Complete Payload Specifications

> Bkz. [Section 5.2](#52-channel-specifications) â€” tÃ¼m payload Ã¶rnekleri ve alan tanÄ±mlarÄ± orada detaylÄ± verilmiÅŸtir.

---

## 12. Error Handling & Edge Cases

### 12.1 Error Codes

| Code | HTTP KarÅŸÄ±lÄ±ÄŸÄ± | AÃ§Ä±klama | Client Aksiyonu |
|---|---|---|---|
| `0` | 200 | Success | â€” |
| `40001` | 400 | Invalid channel name | Channel adÄ±nÄ± dÃ¼zelt |
| `40002` | 400 | Invalid market | Market adÄ±nÄ± kontrol et |
| `40003` | 429 | Too many subscriptions (>200) | Gereksiz sub'larÄ± kaldÄ±r |
| `40004` | 429 | Rate limit exceeded | Slow down |
| `40005` | 400 | Invalid request format | JSON/field validation hatasÄ± |
| `40006` | 400 | Invalid method | Desteklenen method kullan |
| `40100` | 401 | Auth required (private ch) | JWT ile baÄŸlan |
| `40101` | 401 | Token expired | Yeni token al, reconnect |
| `40102` | 403 | Insufficient permissions | Tier upgrade gerekli |
| `50000` | 500 | Internal error | Retry with backoff |
| `50001` | 503 | Service unavailable | Retry with backoff |

### 12.2 Edge Cases

#### E1: Orderbook Sequence Gap

**Senaryo:** Client `seq=100` aldÄ±, sonraki mesaj `seq=102` (101 kayÄ±p).

**Client davranÄ±ÅŸÄ±:**
1. Mevcut local orderbook state'i invalid kabul et
2. `unsubscribe` â†’ `subscribe` (yeni snapshot + sequence sÄ±fÄ±rdan)
3. Veya full reconnect

**Root cause olasÄ±lÄ±klarÄ±:**
- ws-hub'da slow consumer drop
- Network packet loss (nadir, TCP dÃ¼zeltir ama WS frame boundary'de olabilir)

#### E2: Snapshot Timeout

**Senaryo:** Subscribe sonrasÄ± 5 saniye iÃ§inde snapshot gelmez.

**Server davranÄ±ÅŸÄ±:** Snapshot servisi timeout â†’ error response:
```json
{"id": 1, "code": 50000, "msg": "snapshot unavailable, retry"}
```

**Client davranÄ±ÅŸÄ±:** 1-2 saniye bekle, tekrar subscribe et.

#### E3: Kafka Consumer Lag

**Senaryo:** ws-hub Kafka'dan mesaj consume etmekte gecikiyor.

**Mitigation:**
- Alert: Consumer lag > 1000 for > 30s
- Auto-action: Yok (manual investigation gerekir)
- Client impact: Latency artÄ±ÅŸÄ±, ama mesaj kaybÄ± yok (Kafka persistent)

#### E4: Pod Restart / Rolling Update

**Senaryo:** ws-hub pod'u restart ediliyor.

**Flow:**
1. Pod graceful shutdown baÅŸlar
2. TÃ¼m baÄŸlÄ± client'lara `disconnecting` event (code 1001, retryAfterMs: 0)
3. WebSocket close frame gÃ¶nderilir
4. Pod terminates
5. Client'lar KrakenD Ã¼zerinden baÅŸka pod'a reconnect
6. Sticky routing: Yeni pod assignment (consistent hash ring gÃ¼ncellenir)

**Downtime per client:** < 2 saniye (reconnect sÃ¼resi)

#### E5: JWT Expire During Active Connection

**Senaryo:** JWT 24 saat sonra expire olur, connection hala aktif.

**DavranÄ±ÅŸ (Ã–ncesi):** Connection korunur. JWT sadece connection kurulumunda validate edilir. Mevcut baÄŸlantÄ± etkilenmez.

**Problemler Tespit Edilen (Review):**
- User token revoke edilmiÅŸ (account compromised) ama connection aÃ§Ä±k kalÄ±r
- Security risk: 24 saat boyunca unauthorized user veri alabilir

**Ã‡Ã¶zÃ¼m (Yeni):**
1. **Token revocation propagation:** JWT revoke edildiÄŸinde KrakenD â†’ ws-hub'a notification
2. **ws-hub admin API:** `POST /admin/disconnect?userId=u_12345&reason=token_revoked`
3. **Graceful disconnect:** Ä°lgili user'lar `code=4003` (auth failed) ile disconnect edilir

**Implementation (Phase 2):**
- [ ] KrakenD webhook â†’ ws-hub
- [ ] ws-hub admin API port (internal, pod-to-pod)
- [ ] Force disconnect logic: User'Ä±n tÃ¼m pod'lardaki connection'larÄ± terminate eder

> ğŸ“‹ **Review Notu:** JWT revoke sÄ±rasÄ±nda long-lived connection'lar etkilenmemesi security riski tespit edilmiÅŸ (M6). **Ã‡Ã¶zÃ¼m:** Token revocation propagation mechanism tanÄ±mlanmÄ±ÅŸtÄ±r (Phase 2'de implement edilecek).

#### E6: Duplicate Subscribe

**Senaryo:** Client aynÄ± channel'a iki kez subscribe olur.

**DavranÄ±ÅŸ:** Ä°dempotent. Ä°kinci subscribe â†’ success (zaten subscribed). Duplicate mesaj gÃ¶nderilmez.

#### E7: Network Partition (Client â†” Server)

**Senaryo:** Client hala connection aÃ§Ä±k sanÄ±yor ama server tarafÄ± closed.

**Detection:**
- Client: ping gÃ¶nder, 10 saniye pong gelmezse â†’ dead connection
- Server: 60 saniye client'tan mesaj yoksa â†’ connection close

---

## 13. Rate Limiting & Fair Usage

### 13.1 Connection Limits

| Limit | Anonymous | Retail | MM |
|---|---|---|---|
| Max connections per IP | 5 | â€” | â€” |
| Max connections per user | â€” | 10 | 50 |
| Max channels per connection | 50 | 200 | 200 |
| Max total channels per user | â€” | 500 | 1,000 |

### 13.2 Request Rate Limits

| Action | Limit | Window |
|---|---|---|
| `subscribe` requests | 10 | per second, per connection |
| `unsubscribe` requests | 10 | per second, per connection |
| `ping` requests | 5 | per second, per connection |
| Total inbound messages | 20 | per second, per connection |

### 13.3 Rate Limit Enforcement

Rate limit aÅŸÄ±ldÄ±ÄŸÄ±nda:

1. **Soft limit (ilk aÅŸÄ±m):** Error response dÃ¶ner:
   ```json
   {"id": 5, "code": 40004, "msg": "rate limit exceeded, slow down"}
   ```

2. **Hard limit (sÃ¼rekli aÅŸÄ±m, 30s iÃ§inde 10x soft limit):** Graceful disconnect:
   ```json
   {"event": "disconnecting", "code": 4029, "msg": "rate limit exceeded", "retryAfterMs": 30000}
   ```

### 13.4 Fair Usage Policy

- Tek user tÃ¼m pod kaynaklarÄ±nÄ±n >10%'unu kullanamaz
- Anomali tespiti: Bir user'Ä±n message throughput'u ortalamanÄ±n 100x Ã¼stÃ¼ndeyse â†’ alert
- MM tier kullanÄ±cÄ±lar iÃ§in dedicated capacity reservation (opsiyonel, Phase 2)

---

## 14. Testing Strategy

### 14.1 Unit Tests

| BileÅŸen | Test Coverage Hedefi | Focus |
|---|---|---|
| Channel Router | >90% | Subscribe/unsubscribe, fan-out, partial success |
| Sequence Manager | >95% | Monotonicity, gap buffer, overflow |
| Connection Manager | >90% | Auth, rate limit, max connections |
| Write Scheduler | >85% | Backpressure, batching, slow consumer |
| Snapshot Cache | >90% | Orderbook reconstruction, staleness |
| Protocol Parser | >95% | Valid/invalid JSON, edge cases |

### 14.2 Integration Tests

| Test | AÃ§Ä±klama |
|---|---|
| E2E Subscribe Flow | Connect â†’ subscribe â†’ receive data â†’ unsubscribe â†’ disconnect |
| Auth Flow | Anonymous public, JWT private, expired token, invalid token |
| Orderbook Consistency | Subscribe â†’ snapshot â†’ deltas â†’ local OB matches match engine |
| Reconnection Resume | Connect â†’ get seq â†’ disconnect â†’ reconnect with lastSeq â†’ verify no gap |
| Multi-pod Routing | User connects to pod A, private message routed correctly |
| Rate Limit | Exceed subscribe rate â†’ error â†’ exceed hard limit â†’ disconnect |
| Slow Consumer | Client stops reading â†’ drop counter increases â†’ warning â†’ disconnect |
| Graceful Shutdown | Kill pod â†’ clients receive disconnecting event â†’ reconnect to new pod |

### 14.3 Load Tests

| Scenario | Target | Tool |
|---|---|---|
| Max connections | 10K per pod, 100K cluster | k6 WebSocket, custom Go client |
| Message throughput | 500K msg/s per pod outbound | Custom Go benchmark |
| Spike subscribe | 1000 subscribe requests in 1 second | k6 |
| Connection churn | 1000 connect/disconnect per second | Custom |
| Large orderbook | 10,000 level orderbook snapshot delivery | Custom |
| Long-running | 24 hour sustained load | k6 + custom |

### 14.4 Chaos Tests

| Test | AÃ§Ä±klama |
|---|---|
| Kafka broker kill | 1/3 broker down â†’ verify no message loss, consumer rebalance |
| ws-hub pod kill | Random pod termination â†’ verify client reconnect, no data loss |
| Network partition | iptables rules â†’ verify timeout behavior |
| Memory pressure | Limit pod memory â†’ verify OOM behavior, graceful degradation |
| CPU throttle | Limit CPU â†’ verify latency degradation characteristics |

### 14.5 Compatibility Tests

| Client | Test |
|---|---|
| Python `websockets` | Full protocol test |
| Node.js `ws` | Full protocol test |
| Go `gorilla/websocket` | Full protocol test |
| Browser native WebSocket | Public channels test |
| wscat | Manual smoke test |

---

## 15. Rollout Plan

### Phase 0: Infrastructure (Week -2 to 0)

- [ ] Kafka topic'leri oluÅŸtur (`ws.ticker`, `ws.orderbook.delta`, vb.)
- [ ] Match engine'den yeni topic'lere publish baÅŸlat (mevcut topic'lere paralel)
- [ ] KrakenD `/v2` route config hazÄ±rla (disabled)
- [ ] ws-hub v2 Docker image build pipeline
- [ ] Monitoring dashboard + alerting rules

### Phase 1: Internal Beta (Week 0-2)

- [ ] ws-hub v2 deploy (2 pod, staging)
- [ ] KrakenD `/v2` route enable (staging)
- [ ] Internal QA: Protocol compliance test
- [ ] Internal QA: Orderbook consistency test (48 saat)
- [ ] Performance baseline (latency, throughput)
- [ ] Fix bugs, iterate

### Phase 2: MM Beta (Week 2-4)

- [ ] Production deploy (3 pod, low traffic)
- [ ] KrakenD `/v2` route enable (production, whitelist IP)
- [ ] 2-3 MM partner onboard
- [ ] MM feedback collection (1:1 calls)
- [ ] Orderbook consistency monitoring (production, 7 gÃ¼n)
- [ ] Sequence gap alert: zero tolerance
- [ ] Fix issues, iterate

### Phase 3: Public GA (Week 4-8)

- [ ] KrakenD `/v2` route open (all users)
- [ ] Scale to 5-10 pods
- [ ] Documentation publish
- [ ] Python SDK release
- [ ] TypeScript SDK release
- [ ] Blog post / changelog announcement
- [ ] Monitor adoption metrics

### Phase 4: Migration Push (Week 8-12)

- [ ] v1 deprecation header ekle
- [ ] v1 kullanÄ±cÄ±larÄ±na email bildirim
- [ ] docs.paribu.com v1 deprecation banner
- [ ] v1 connection limit'i kademeli dÃ¼ÅŸÃ¼r
- [ ] v2 adoption tracking (target: %80)

### Phase 5: v1 Sunset (Week 12-24)

- [ ] v1 yeni connection kabul etmeyi durdur (week 20)
- [ ] v1 mevcut connection'larÄ± graceful disconnect (week 24)
- [ ] v1 pod'larÄ± decommission (week 28, 4 hafta cold standby sonrasÄ±)

### Rollback Plan

Her phase'de rollback mÃ¼mkÃ¼n:
- Phase 1-2: ws-hub v2 pod'larÄ± sil, KrakenD route disable
- Phase 3: `/v2` route disable, announcement
- Phase 4-5: v1 deprecation header kaldÄ±r, limit'leri geri al

---

## 16. Success Metrics & KPIs

### 16.1 Adoption Metrics

| Metrik | Target (3 ay) | Target (6 ay) |
|---|---|---|
| v2 aktif connection sayÄ±sÄ± | 5,000 | 20,000 |
| v2'ye geÃ§en MM sayÄ±sÄ± | 5 | TÃ¼m aktif MM'ler |
| v1 kullanÄ±m oranÄ± | <%50 | %0 (sunset) |
| 3rd party integratÃ¶r geÃ§iÅŸi | %30 | %90 |

### 16.2 Performance Metrics

| Metrik | Target |
|---|---|
| E2E message latency p50 | <5ms |
| E2E message latency p99 | <25ms |
| Snapshot delivery p99 | <50ms |
| Uptime | >99.95% |
| Message delivery rate | >99.99% (drop <0.01%) |

### 16.3 Reliability Metrics

| Metrik | Target |
|---|---|
| Sequence gap events per day | 0 |
| Orderbook desync incidents per month | 0 |
| Mean time to reconnect (MTTR) | <2s |
| Graceful shutdown success rate | >99% |

### 16.4 Business Metrics

| Metrik | Target |
|---|---|
| MM onboarding time | <2 gÃ¼n (vs mevcut ~2 hafta) |
| MM-sourced liquidity artÄ±ÅŸÄ± | %20 (6 ay) |
| API-related support ticket azalmasÄ± | %50 |
| New MM partner acquisition | +3 (6 ay) |

---

## 17. Open Questions / Risks

### 17.1 Open Questions

| # | Soru | Sahip | Status | Review Ã‡Ã¶zÃ¼mÃ¼ |
|---|---|---|---|---|
| Q1 | Private topic routing: Sticky hash mÄ±, broadcast mÄ±, internal relay mi? | Platform Eng | âœ… **KARAR VERÄ°LDÄ°** | **Sticky hash via KrakenD** (Opsiyon 3 approved). Pod scale event'te graceful reconnect mechanism eklenmiÅŸtir. |
| Q2 | Sequence number public channel'larda Kafka offset mi, baÄŸÄ±msÄ±z counter mi? | Platform Eng | âœ… **KARAR VERÄ°LDÄ°** | **Public: Kafka offset = sequence** (pod-independent, persistent). **Private: Pod-local counter** (pod restart â†’ snapshot fallback). |
| Q3 | API key authentication â€” v2 GA'da mÄ±, sonra mÄ±? | Product | â³ **Phase 2 feedback'e baÄŸlÄ±** | Phase 2 MM beta'da JWT test edip MM'ler ÅŸikayet ederse (token refresh overhead) â†’ Phase 3'te API key ekle. |
| Q4 | `positions` channel ne zaman aktifleÅŸecek? | Product | âœ… **KARAR VERÄ°LDÄ°** | **Futures launch'a kadar placeholder** (client subscribe â†’ 40002 error). |
| Q5 | SDK dilleri? Python + TypeScript yeterli mi? | Product | âœ… **KARAR VERÄ°LDÄ°** | **v2 GA: Python + TypeScript**. **Phase 3: Go SDK** (MM feedback'ine gÃ¶re). Java/C# gerek olursa Phase 4'te. |
| Q6 | MM'ler iÃ§in dedicated pod pool gerekli mi? | Platform Eng | â³ **Phase 2 test sonrasÄ±** | Phase 2'de shared pool ile baÅŸla. MM'ler latency problemi yaÅŸarsa dedicated pool ekle (ws-hub-v2-mm deployment). |
| Q7 | Message batching (1ms) â€” default aÃ§Ä±k mÄ± kapalÄ± mÄ±? | Platform Eng | âœ… **KARAR VERÄ°LDÄ°** | **Default kapalÄ±** (latency-sensitive MM'ler iÃ§in). `enableForTier: ["retail"]` (retail tier'da opsiyonel aÃ§). |
| Q8 | KrakenD WebSocket proxy yeterli mi? | Platform Eng | â³ **PoC sonrasÄ± (Phase 0)** | **PoC test edilmeli:** KrakenD 1 pod @ 10K concurrent connection + TLS â†’ CPU/memory profiling. BaÅŸarÄ±lÄ± ise KrakenD kullan. Fail ise custom Go proxy (2-3 hafta dev). |
| Q9 | Disaster Recovery planÄ±? (Review'da eklendi) | Platform Eng | â³ **Phase 4-5'te deÄŸer** | Single-region (Ä°stanbul) ile start. Phase 4-5'te DR (cross-region replication + passive standby) planlanabilir. AyrÄ± PRD gerekebilir. |

### 17.2 Risks

| # | Risk | OlasÄ±lÄ±k | Etki | Mitigation |
|---|---|---|---|---|
| R1 | KrakenD WebSocket proxy performans bottleneck | Orta | YÃ¼ksek | PoC ile erken test. Gerekirse KrakenD bypass, direct TCP/TLS termination |
| R2 | In-memory orderbook reconstruction memory pressure | DÃ¼ÅŸÃ¼k | Orta | Market baÅŸÄ±na OB boyut limiti. Monitoring + alerting. Lazy load (sadece subscribe olan market'ler) |
| R3 | Sticky routing ile pod restart'ta connection storm | Orta | Orta | Graceful shutdown + staggered reconnect (retryAfterMs randomization) |
| R4 | Kafka Redpanda topic sayÄ±sÄ± artÄ±ÅŸÄ± (per-market partition) | DÃ¼ÅŸÃ¼k | DÃ¼ÅŸÃ¼k | Redpanda partition limitleri geniÅŸ. 500 market Ã— 6 topic = 3000 partition (manageable) |
| R5 | MM'lerin v2'ye geÃ§mek istememesi (v1 "yeterli" algÄ±sÄ±) | Orta | YÃ¼ksek | Beta'da MM'lerle yakÄ±n Ã§alÄ±ÅŸma. Feature deÄŸer Ã¶nerisi net olmalÄ±. v1 sunset timeline aÃ§Ä±k |
| R6 | Sequence gap buffer memory (5 dk Ã— yÃ¼ksek throughput market) | DÃ¼ÅŸÃ¼k | Orta | Per-channel buffer size cap. Oldest-first eviction. Memory monitoring |
| R7 | Gorilla WebSocket library maintenance durumu | DÃ¼ÅŸÃ¼k | DÃ¼ÅŸÃ¼k | `nhooyr.io/websocket` veya `gobwas/ws` alternatif olarak evaluate |
| R8 | Multi-pod private channel routing complexity | Orta | YÃ¼ksek | Sticky routing ile basitleÅŸtir. PoC ile validate |

---

## 18. Competitive Analysis

### 18.1 Feature Comparison

| Feature | Binance | Bybit | OKX | Paribu v1 | **Paribu v2** |
|---|---|---|---|---|---|
| **Endpoint** | `stream.binance.com/ws` | `stream.bybit.com/v5/public` | `ws.okx.com:8443/ws/v5` | `stream.paribu.com` | `stream.paribu.com/v2` |
| **Protocol** | JSON | JSON | JSON | JSON | JSON |
| **Subscribe format** | `{"method":"SUBSCRIBE","params":["btcusdt@ticker"]}` | `{"op":"subscribe","args":["orderbook.50.BTCUSDT"]}` | `{"op":"subscribe","args":[{"channel":"tickers","instId":"BTC-USDT"}]}` | N/A (auto) | `{"method":"subscribe","params":["ticker@btc_tl"]}` |
| **Sequence numbers** | âŒ (lastUpdateId for OB only) | âœ… | âŒ (OB: seqId only) | âŒ | âœ… (all channels) |
| **Snapshot on subscribe** | âŒ (REST API gerekli) | âœ… (orderbook) | âŒ (REST API gerekli) | âŒ | âœ… (orderbook + more) |
| **Client ping/pong** | âŒ (server-only) | âœ… (`{"op":"ping"}`) | âœ… (`ping` frame) | âŒ | âœ… |
| **Incremental OB** | âœ… (diff stream) | âœ… (delta) | âœ… (snapshot + delta) | âŒ | âœ… (delta + snapshot depth) |
| **BBO channel** | âœ… (`bookTicker`) | âœ… (`tickers`) | âœ… (`bbo-tbt`) | âŒ | âœ… |
| **Kline/candle** | âœ… | âœ… | âœ… | âŒ | âœ… |
| **Private channels** | âœ… (listenKey) | âœ… (auth) | âœ… (login) | âœ… (authorize frame) | âœ… (JWT via KrakenD) |
| **Max connections** | 5/IP (stream) | 20/IP (public) | 3/IP/channel | 100/client | 5/IP (pub), 10/user (priv) |
| **Max channels** | 200/conn | 10/conn (!) | 240 (public) | 16 total | 200/conn |
| **Auth method** | HMAC listenKey (REST) | HMAC WS login | HMAC WS login | JWT (authorize frame) | JWT (HTTP header) |
| **Reconnection resume** | âŒ | âŒ | âŒ | âŒ | âœ… (lastSeq) |
| **Graceful disconnect** | âŒ | âŒ | âŒ | âŒ | âœ… (code + retryAfter) |
| **Rate limit info** | 429 HTTP (REST), disconnect (WS) | Error frame | Error frame | âŒ | âœ… (error frame + code) |
| **Compression** | âŒ (stream), âœ… (api) | âœ… (deflate) | âœ… (deflate) | âœ… (deflate) | âœ… (deflate) |
| **Binary protocol** | âŒ | âŒ | âŒ | âŒ | âŒ (v3 candidate) |

### 18.2 Paribu v2 AvantajlarÄ±

| Avantaj | vs Binance | vs Bybit | vs OKX |
|---|---|---|---|
| **All-channel sequence numbers** | âœ… (Binance sadece OB) | EÅŸit | âœ… (OKX sadece OB) |
| **Snapshot on subscribe** | âœ… (Binance REST gerektirir) | EÅŸit | âœ… (OKX REST gerektirir) |
| **Reconnection resume (lastSeq)** | âœ… | âœ… | âœ… |
| **Graceful disconnect** | âœ… | âœ… | âœ… |
| **200 channel/conn** | EÅŸit | âœ… (Bybit 10!) | EÅŸit |
| **JWT (no handshake auth)** | âœ… (Binance listenKey REST call) | âœ… (Bybit WS login) | âœ… (OKX WS login) |
| **Connection ID for debug** | âœ… | âœ… | âœ… |

### 18.3 Paribu v2 DezavantajlarÄ± / Farklar

| Konu | Durum | Plan |
|---|---|---|
| Binance'Ä±n market depth'i Ã§ok daha derin | Likidite farkÄ±, API sorunu deÄŸil | MM onboarding ile likidite artÄ±ÅŸÄ± |
| Combined stream URL (Binance `/stream?streams=`) | Paribu v2 subscribe ile | v2 yaklaÅŸÄ±mÄ± daha esnek (runtime subscribe/unsub) |
| OKX'in instrument-based channel yapÄ±sÄ± | Paribu v2 `@market` formatÄ± daha basit | â€” |
| WS Ã¼zerinden order placement | Yok (REST only) | AyrÄ± PRD (v3 candidate) |
| FIX protocol | Yok | Talep gelirse deÄŸerlendirilecek |

### 18.4 Ã–nemli Rakip DetaylarÄ±

**Binance:**
- listenKey (REST'ten alÄ±nÄ±r, 60dk expire, PUT ile uzatÄ±lÄ±r) â†’ ek complexity
- Orderbook: REST snapshot + WS delta â†’ client tarafÄ±nda senkronizasyon gerekli
- 5 connection limit (stream), 300 subscription limit (combined)
- 24 saat sonra auto-disconnect

**Bybit:**
- Per-connection 10 topic limit (!) â†’ birden fazla connection aÃ§mak zorunlu
- Orderbook: Subscribe sonrasÄ± snapshot + delta (Paribu v2 ile aynÄ±)
- 100 connection limit per IP
- Ping every 20s zorunlu (yoksa disconnect)

**OKX:**
- WS login (HMAC) â†’ her connection'da auth handshake
- 3 connection per IP per channel type (Ã§ok kÄ±sÄ±tlÄ±)
- Orderbook: REST snapshot gerekli (Binance gibi)
- 240 subscription limit (public)

---

## Appendix A: SDK Ã–rnekleri

### Python

```python
import asyncio
import json
import websockets

async def main():
    uri = "wss://stream.paribu.com/v2"
    headers = {"Authorization": "Bearer <JWT>"}

    async with websockets.connect(uri, extra_headers=headers) as ws:
        # Connected event
        msg = json.loads(await ws.recv())
        print(f"Connected: {msg['connId']}")

        # Subscribe
        await ws.send(json.dumps({
            "method": "subscribe",
            "params": ["ticker@btc_tl", "orderbook@btc_tl", "orders@btc_tl"],
            "id": 1
        }))

        # Ack
        ack = json.loads(await ws.recv())
        print(f"Subscribed: {ack['data']['subscribed']}")

        # Listen
        last_seq = {}
        async for raw in ws:
            msg = json.loads(raw)
            if "ch" in msg:
                ch = msg["ch"]
                seq = msg["seq"]

                # Gap detection
                if ch in last_seq and seq != last_seq[ch] + 1:
                    print(f"GAP on {ch}: expected {last_seq[ch]+1}, got {seq}")
                    # Resubscribe to get fresh snapshot
                    await ws.send(json.dumps({
                        "method": "unsubscribe", "params": [ch], "id": 99
                    }))
                    await ws.send(json.dumps({
                        "method": "subscribe", "params": [ch], "id": 100
                    }))

                last_seq[ch] = seq
                print(f"[{ch}] seq={seq} data={msg['data']}")

asyncio.run(main())
```

### TypeScript / Node.js

```typescript
import WebSocket from 'ws';

const ws = new WebSocket('wss://stream.paribu.com/v2', {
  headers: { Authorization: 'Bearer <JWT>' }
});

const lastSeq: Record<string, number> = {};

ws.on('open', () => {
  ws.send(JSON.stringify({
    method: 'subscribe',
    params: ['ticker@btc_tl', 'orderbook@btc_tl', 'orders@btc_tl'],
    id: 1
  }));
});

ws.on('message', (raw: Buffer) => {
  const msg = JSON.parse(raw.toString());

  if (msg.event === 'connected') {
    console.log(`Connected: ${msg.connId}`);
    return;
  }

  if (msg.id && msg.code !== undefined) {
    console.log(`Ack id=${msg.id}: ${msg.msg}`);
    return;
  }

  if (msg.ch) {
    const { ch, seq, data } = msg;

    // Gap detection
    if (lastSeq[ch] !== undefined && seq !== lastSeq[ch] + 1) {
      console.warn(`GAP on ${ch}: expected ${lastSeq[ch] + 1}, got ${seq}`);
    }

    lastSeq[ch] = seq;
    console.log(`[${ch}] seq=${seq}`, data);
  }
});

// Ping every 20s
setInterval(() => {
  ws.send(JSON.stringify({ method: 'ping', id: Date.now() }));
}, 20_000);
```

---

## Appendix B: Glossary

| Terim | AÃ§Ä±klama |
|---|---|
| **BBO** | Best Bid/Offer â€” en iyi alÄ±ÅŸ ve satÄ±ÅŸ fiyatÄ± |
| **Delta** | Orderbook'ta sadece deÄŸiÅŸen seviyeleri iÃ§eren gÃ¼ncelleme |
| **Fan-out** | Tek mesajÄ±n birden fazla subscriber'a daÄŸÄ±tÄ±lmasÄ± |
| **Gap detection** | Sequence number'da atlama (kayÄ±p mesaj) tespiti |
| **KrakenD** | API Gateway (L7 reverse proxy, JWT validation, rate limiting) |
| **MM (Market Maker)** | Borsada sÃ¼rekli alÄ±ÅŸ/satÄ±ÅŸ emirleri vererek likidite saÄŸlayan katÄ±lÄ±mcÄ± |
| **Orderbook** | Bir market'teki tÃ¼m aÃ§Ä±k alÄ±ÅŸ/satÄ±ÅŸ emirlerinin fiyat-miktar listesi |
| **Snapshot** | Bir channel'Ä±n mevcut tam durumunun (state) tek seferde gÃ¶nderilmesi |
| **Sequence (seq)** | Per-channel monoton artan mesaj numarasÄ± |
| **Sticky routing** | AynÄ± kullanÄ±cÄ±nÄ±n her zaman aynÄ± backend pod'a yÃ¶nlendirilmesi |
| **Throttle** | Mesaj gÃ¶nderim frekansÄ±nÄ±n sÄ±nÄ±rlandÄ±rÄ±lmasÄ± |
| **ws-hub** | Paribu'nun WebSocket pub/sub relay servisi |

---

## Appendix C: Revision History

| Versiyon | Tarih | Yazar | DeÄŸiÅŸiklik |
|---|---|---|---|
| 1.0 | 2026-02-19 | Platform Engineering | Ä°lk draft |

---

*Bu dokÃ¼man Paribu Platform Engineering ekibi tarafÄ±ndan hazÄ±rlanmÄ±ÅŸtÄ±r. DaÄŸÄ±tÄ±m: Internal â€” Confidential.*


---

# Appendix: Teknik Review



| | |
|---|---|
| **Review DokÃ¼manÄ±** | PRD-2026-003-REVIEW |
| **Kaynak PRD** | PRD-2026-003 (v1.0, 2026-02-19) |
| **Review Tarihi** | 2026-02-21 |
| **Reviewer** | Sub-Agent 973fc087 |
| **Review Tipi** | Zero-Risk Isolation + gRPC Extension Proposal |

---

## Ä°Ã§indekiler

1. [Ã–zet (Executive Summary)](#1-Ã¶zet-executive-summary)
2. [BÃ¶lÃ¼m 1: PRD Review â€” Zero-Risk Ä°zolasyon OdaklÄ±](#2-bÃ¶lÃ¼m-1-prd-review--zero-risk-izolasyon-odaklÄ±)
   - 2.1 [Ä°zolasyon Analizi](#21-izolasyon-analizi)
   - 2.2 [Mimari Feedback](#22-mimari-feedback)
   - 2.3 [AÃ§Ä±k Sorular (Q1-Q8) DeÄŸerlendirmesi](#23-aÃ§Ä±k-sorular-q1-q8-deÄŸerlendirmesi)
   - 2.4 [Risk DeÄŸerlendirmesi (R1-R8)](#24-risk-deÄŸerlendirmesi-r1-r8)
   - 2.5 [Eksik Konular](#25-eksik-konular)
   - 2.6 [Deployment Ä°zolasyon Validasyonu](#26-deployment-izolasyon-validasyonu)
3. [BÃ¶lÃ¼m 2: gRPC Direct Message Delivery Ã–nerisi](#3-bÃ¶lÃ¼m-2-grpc-direct-message-delivery-Ã¶nerisi)
   - 3.1 [Use Case: Neden gRPC?](#31-use-case-neden-grpc)
   - 3.2 [Mimari Entegrasyon](#32-mimari-entegrasyon)
   - 3.3 [API TasarÄ±mÄ± (Proto Sketch)](#33-api-tasarÄ±mÄ±-proto-sketch)
   - 3.4 [Deployment Stratejisi](#34-deployment-stratejisi)
   - 3.5 [Authentication](#35-authentication)
   - 3.6 [Avantajlar (vs WebSocket)](#36-avantajlar-vs-websocket)
   - 3.7 [Trade-offs](#37-trade-offs)
   - 3.8 [Ã–neri: Faz ve Timeline](#38-Ã¶neri-faz-ve-timeline)

---

## 1. Ã–zet (Executive Summary)

Bu dokÃ¼man, WS v2 Stream API PRD'yi (PRD-2026-003) iki kritik aÃ§Ä±dan incelemektedir:

### Review BÃ¶lÃ¼m 1: Zero-Risk Ä°zolasyon Analizi
PRD'nin **en kritik gereksinimi**, v2'nin v1 Ã¼zerinde **sÄ±fÄ±r etki** yapmasÄ± ve izole Ã§alÄ±ÅŸmasÄ±dÄ±r. Review sonuÃ§larÄ±:

**âœ… GÃ¼Ã§lÃ¼ Ä°zolasyon NoktalarÄ±:**
- AyrÄ± KrakenD route (`/v2`)
- AyrÄ± deployment (ws-hub-v2 pod'larÄ±)
- AyrÄ± Kafka consumer group (`ws-hub-v2`)
- v1 ve v2 farklÄ± pod set'leri Ã¼zerinde

**âš ï¸ Ä°zolasyon Riski Tespit Edilen Alanlar:**
1. **Kafka topic paylaÅŸÄ±mÄ±** â€” Match engine aynÄ± topic'lere yazÄ±yor, consumer group farklÄ± ama partition load v2 launch ile artabilir
2. **KrakenD shared instance** â€” PRD'de "KrakenD Gateway" tek katmandan bahsediyor, v1 ve v2 aynÄ± KrakenD instance kullanÄ±yorsa CPU/memory contention riski
3. **Snapshot Service baÄŸÄ±mlÄ±lÄ±ÄŸÄ±** â€” v2'nin orderbook snapshot iÃ§in external gRPC service'e baÄŸÄ±mlÄ±lÄ±ÄŸÄ±, ancak bu servis v1 tarafÄ±ndan da kullanÄ±lÄ±yorsa bottleneck
4. **Redis optional kullanÄ±mÄ±** â€” PRD'de Redis "opt." olarak iÅŸaretli, eÄŸer paylaÅŸÄ±lÄ±yorsa (Ã¶rneÄŸin JWT blacklist) izolasyon zayÄ±flar

**Genel DeÄŸerlendirme:** Ä°zolasyon konsepti **saÄŸlam**, ancak **operasyonel ve deployment detaylarÄ±** netleÅŸtirilmeli. 

**Action Items:**
- Kafka partition'larÄ±n v2 eklenmesiyle v1'e etkisinin Ã¶lÃ§Ã¼lmesi (load test)
- KrakenD instance'Ä±n v1/v2 iÃ§in ayrÄ± mÄ± ortak mÄ± olacaÄŸÄ±nÄ±n aÃ§Ä±kÃ§a belirtilmesi
- Snapshot Service kapasitesinin v2 yÃ¼kÃ¼ ile test edilmesi
- Redis kullanÄ±mÄ±nÄ±n scope'unun netleÅŸtirilmesi

### Review BÃ¶lÃ¼m 2: gRPC Streaming Ã–nerisi
PRD kapsamÄ±na **ek bir delivery mechanism** olarak **gRPC bidirectional streaming** Ã¶nerilmektedir.

**Hedef kitle:** MM bot'larÄ± (server-side Ã§alÄ±ÅŸan, browser constraint'i olmayan)

**Temel avantajlar:**
- Native protobuf â†’ %40-60 daha dÃ¼ÅŸÃ¼k bandwidth
- Built-in flow control (backpressure)
- HTTP/2 multiplexing â†’ single TCP connection, multiple streams
- DÃ¼ÅŸÃ¼k latency (no JSON parse overhead)

**Ã–neri:** **Phase 3** (week 12-24) â€” WebSocket GA'dan sonra, MM feedback ile Ã¶ncelik belirlenecek ÅŸekilde.

---

## 2. BÃ¶lÃ¼m 1: PRD Review â€” Zero-Risk Ä°zolasyon OdaklÄ±

### 2.1 Ä°zolasyon Analizi

#### 2.1.1 âœ… Ä°ZOLE EDÄ°LMÄ°Å BÃ–LGELER

| Kaynak | Ä°zolasyon Durumu | KanÄ±t (PRD Referans) |
|---|---|---|
| **KrakenD Route** | âœ… Tam izole | Section 7.1: `/v2` route; Section 8.2: AyrÄ± endpoint `stream.paribu.com/v2` |
| **Pod Deployment** | âœ… Tam izole | Section 7.5: "ws-hub v2 (5-20 pod, HPA)", Section 15 Phase 1: "ayrÄ± pod set" |
| **Kafka Consumer Group** | âœ… Tam izole | Section 7.4: Consumer group `ws-hub-v2` (v1'den farklÄ±) |
| **Connection Manager** | âœ… Tam izole | Section 7.3.1: Her pod kendi `map[connId]*Connection` tutuyor |
| **Write Scheduler** | âœ… Tam izole | Section 7.3.5: Per-pod, per-connection write channel |

**SonuÃ§:** Core business logic ve runtime state tamamen izole. v2 pod crash olursa v1 etkilenmez.

---

#### 2.1.2 âš ï¸ PAYLAÅILAN KAYNAKLAR (Potansiyel Ä°zolasyon Riski)

##### **R1.1 â€” Kafka Topic PaylaÅŸÄ±mÄ±**

**Durum:** v1 ve v2, **aynÄ± Kafka topic'lerden** consume ediyor:
- `ws.ticker`, `ws.orderbook.delta`, `ws.trades`, vb.

**PRD Referans:** Section 7.4 â€” Topic listesi belirtilmiÅŸ, ancak v1'in aynÄ± topic'leri kullandÄ±ÄŸÄ± PRD'de aÃ§Ä±kÃ§a belirtilmemiÅŸ. Section 15 Phase 0: "Match engine'den yeni topic'lere publish baÅŸlat (mevcut topic'lere **paralel**)" â†’ Bu belirsiz: "paralel" = aynÄ± topic'e mi yoksa yeni topic'lere de mi?

**Ä°zolasyon Riski:**
- Kafka broker'da partition count artarsa (v2'nin yeni consumer group'u ekleniyor) â†’ broker CPU/disk I/O artar
- v2 launch sonrasÄ± consumer rebalance (Kafka consumer group koordinasyonu) â†’ broker yÃ¼kÃ¼ artabilir
- Match engine'in publish throughput'u artarsa (v1 + v2 consumer var) â†’ broker bant geniÅŸliÄŸi

**Risk Seviyesi:** **DÃœÅÃœK-ORTA**
- Kafka/Redpanda 3 node cluster, modern hardware â†’ 2 consumer group ek yÃ¼k dÃ¼ÅŸÃ¼k
- Ancak match engine publish rate Ã§ok yÃ¼ksekse (Ã¶rneÄŸin 100K msg/s Ã— 6 topic) â†’ dikkat

**Mitigation Kontrol:**
- PRD Section 15 Phase 0: Match engine "mevcut topic'lere paralel" diyor â†’ **netleÅŸtirilmeli**: aynÄ± topic'e mi? Yoksa v1 ve v2 iÃ§in ayrÄ± topic'ler mi?
- **Ã–nerilen yaklaÅŸÄ±m:** AynÄ± topic'leri kullan (kaynak tasarrufu), ancak v2 launch Ã¶ncesi **Kafka load test** yapÄ±lmalÄ± (v1 + v2 consumer simÃ¼lasyonu)
- **Alternatif (tam izolasyon):** v2 iÃ§in ayrÄ± topic'ler (`ws.v2.ticker`, `ws.v2.orderbook.delta`) â†’ Match engine her iki set'e de yazmalÄ± â†’ daha fazla Kafka storage ve network, ama izolasyon maksimum

**Action Item:**
```
[ ] Kafka capacity planning: Match engine publish rate Ã— 2 consumer group ile load test
[ ] PRD'de topic stratejisi aÃ§Ä±k hale getirilmeli (shared vs separate)
[ ] v2 launch monitoring: Kafka broker CPU, disk I/O, replication lag
```

---

##### **R1.2 â€” KrakenD Gateway Instance**

**Durum:** PRD Section 7.1 ve 7.5'te "KrakenD Gateway" tek katman olarak gÃ¶steriliyor. v1 ve v2 **aynÄ± KrakenD instance** kullanÄ±yor mu yoksa ayrÄ± mÄ±?

**PRD Referans:** Section 7.5: "KrakenD (3 pod)" â€” sayÄ± belirtilmiÅŸ ama v1/v2 ayrÄ±mÄ± yok.

**Ä°zolasyon Riski:**
- **Shared instance ise:** v2 traffic spike â†’ KrakenD CPU/memory tÃ¼kenir â†’ v1 yavaÅŸlar veya error rate artar
- **Rate limiting shared ise:** v2'nin rate limit aÅŸÄ±mÄ±, shared KrakenD instance limitlerini tÃ¼ketebilir
- **JWT validation bottleneck:** v2 launch'ta ani JWT validation artÄ±ÅŸÄ± â†’ KrakenD RS256 CPU yÃ¼kÃ¼

**Risk Seviyesi:** **ORTA-YÃœKSEK**
- KrakenD L7 proxy â†’ CPU-intensive (TLS termination, JWT decode, routing)
- v2 beta/GA geÃ§iÅŸinde traffic 10x artarsa â†’ shared instance sorun yaratabilir

**Mitigation Kontrol:**
- PRD'de aÃ§Ä±kÃ§a belirtilmeli: **AyrÄ± KrakenD instance** (Ã¶rneÄŸin `krakend-v1-*` ve `krakend-v2-*` deployment'larÄ±)
- Veya KrakenD config'de v1 ve v2 route'larÄ± iÃ§in **ayrÄ± rate limit bucket** (per-route rate limiting)

**Ã–nerilen YaklaÅŸÄ±m:**
```yaml
# KrakenD Deployment (Ã¶nerilen)
- krakend-v1 (3 pod) â†’ /ws route â†’ ws-hub (v1)
- krakend-v2 (3 pod) â†’ /v2 route â†’ ws-hub-v2
```

**Veya (maliyet optimizasyonu):**
```yaml
# Shared KrakenD ama strict resource isolation
- krakend (5 pod total)
  - /ws route â†’ ws-hub (rate limit: 10K conn/pod)
  - /v2 route â†’ ws-hub-v2 (rate limit: 10K conn/pod)
  - Resource quota: v2 max %50 CPU/memory kullanabilir
```

**Action Item:**
```
[ ] KrakenD deployment strategy netleÅŸtirilmeli (shared vs separate)
[ ] Shared kullanÄ±lÄ±yorsa, per-route resource quota tanÄ±mlanmalÄ±
[ ] KrakenD load test: v1 + v2 concurrent traffic (20K v1 conn + 10K v2 conn)
```

---

##### **R1.3 â€” Snapshot Service (gRPC)**

**Durum:** PRD Section 7.3.4: "Orderbook snapshot: Match engine'den gRPC ile Ã§ekilir"

**PRD belirsizlik:** Bu "Snapshot Service" nedir? AyrÄ± bir servis mi yoksa match engine'in bir endpoint'i mi? v1 de aynÄ± servisi kullanÄ±yor mu?

**Ä°zolasyon Riski:**
- v2 her subscribe'da snapshot Ã§ekerse (Ã¶rneÄŸin 1000 subscription/s) â†’ Snapshot Service bottleneck
- v1 de aynÄ± servisi kullanÄ±yorsa â†’ v2'nin yÃ¼kÃ¼ v1'i yavaÅŸlatÄ±r

**Risk Seviyesi:** **ORTA**
- Snapshot Service'in kapasitesi bilinmiyor
- PRD'de "In-memory orderbook reconstruction" Ã¶nerilmiÅŸ (Section 7.3.4) â†’ bu durumda external service call yok, risk azalÄ±r

**Mitigation Kontrol:**
- PRD Section 7.3.4: "**Karar: In-memory orderbook reconstruction**" â†’ ws-hub v2 kendi orderbook'unu delta'lardan oluÅŸturuyor
- Bu durumda **external Snapshot Service'e dependency yok** â†’ izolasyon saÄŸlanmÄ±ÅŸ âœ…

**Ancak:**
- Private channel snapshot'larÄ± (orders, balances) iÃ§in "order-api / wallet servisine gRPC call" (Section 7.3.4)
- Bu servisler v1 tarafÄ±ndan da kullanÄ±lÄ±yorsa â†’ load artÄ±ÅŸÄ± v1'i etkileyebilir

**Ã–nerilen YaklaÅŸÄ±m:**
- order-api ve wallet servisleri **zaten distributed** (her request baÄŸÄ±msÄ±z) â†’ stateless
- v2 snapshot call'larÄ±, normal API traffic gibi davranÄ±r â†’ mevcut rate limit ve kapasitede handle edilmeli
- **Action:** order-api ve wallet load test'ine v2 snapshot traffic ekle (Ã¶rneÄŸin +20% request rate)

**Action Item:**
```
[ ] order-api ve wallet servislerinin v2 snapshot yÃ¼kÃ¼nÃ¼ handle edip edemeyeceÄŸi test edilmeli
[ ] v2 snapshot rate limiting eklenmeli (Ã¶rneÄŸin user baÅŸÄ±na 10 subscribe/s)
```

---

##### **R1.4 â€” Redis (Optional Shared Resource)**

**Durum:** PRD Section 7.1 ve 7.5: Redis "opt." (optional) olarak gÃ¶steriliyor.

**PRD'de Redis kullanÄ±m senaryolarÄ±:**
- Section 7.3.4: Trades snapshot iÃ§in "Redis'ten" (alternatif olarak)
- Section 9.2: JWT blacklist (revocation) â€” KrakenD kontrol eder

**Ä°zolasyon Riski:**
- **JWT blacklist Redis shared ise:** v2'nin revoke request'leri Redis'i yavaÅŸlatÄ±r â†’ v1 JWT validation yavaÅŸlar
- **Trades snapshot Redis shared ise:** v2 snapshot query'leri Redis CPU/memory tÃ¼ketir â†’ v1 etkilenir

**Risk Seviyesi:** **DÃœÅÃœK**
- JWT blacklist: Ã‡ok dÃ¼ÅŸÃ¼k throughput (revoke rare event)
- Trades snapshot: Ä°lk subscribe'da bir kez Ã§aÄŸrÄ±lÄ±r, sonrasÄ± yok

**Mitigation Kontrol:**
- Redis cluster veya keyspace separation kullanÄ±labilir (Ã¶rneÄŸin `v1:*` ve `v2:*` prefix'leri)
- Redis sentinel/cluster â†’ multiple instances, load distribution

**Ã–nerilen YaklaÅŸÄ±m:**
- JWT blacklist: Shared Redis kullanÄ±labilir (revoke rate Ã§ok dÃ¼ÅŸÃ¼k)
- Trades snapshot: ws-hub v2 **in-memory cache** kullanmalÄ± (son 50 trade'i memory'de tut) â†’ Redis'e baÄŸÄ±mlÄ±lÄ±k kaldÄ±rÄ±lÄ±r

**Action Item:**
```
[ ] Redis kullanÄ±m scope'unu netleÅŸtir (sadece JWT blacklist mÄ± yoksa snapshot cache de mi?)
[ ] ws-hub v2'de trades snapshot iÃ§in in-memory cache kullan (Redis dependency ortadan kalksÄ±n)
```

---

#### 2.1.3 Ä°ZOLASYON Ã–ZET SKORU

| Kaynak Tipi | Ä°zolasyon Durumu | Risk Seviyesi | Action Gerekli? |
|---|---|---|---|
| **Pod Deployment** | âœ… Tam izole | YOK | HayÄ±r |
| **Kafka Consumer Group** | âœ… Tam izole | YOK | HayÄ±r |
| **KrakenD Route** | âœ… Tam izole | YOK | HayÄ±r |
| **Kafka Topic** | âš ï¸ Shared | DÃœÅÃœK-ORTA | Evet (load test) |
| **KrakenD Instance** | â“ Belirsiz | ORTA-YÃœKSEK | Evet (aÃ§Ä±klama gerekli) |
| **Snapshot Service (gRPC)** | âš ï¸ Shared (order-api, wallet) | ORTA | Evet (capacity test) |
| **Redis** | âš ï¸ Optional shared | DÃœÅÃœK | Evet (scope netleÅŸtir) |

**GENEL DEÄERLENDÄ°RME:**
- **Core izolasyon: GÃœÃ‡LÃœ** âœ…
- **Shared resource management: Ä°YÄ°LEÅTÄ°RÄ°LEBÄ°LÄ°R** âš ï¸
- **v1'e sÄ±fÄ±r etki garantisi: %85** (kalan %15 shared resource'larÄ±n capacity planlamasÄ± ile saÄŸlanÄ±r)

---

### 2.2 Mimari Feedback

#### 2.2.1 âœ… GÃ¼Ã§lÃ¼ Noktalar

1. **Sequence Number Everywhere (Section 5.4)**
   - Public ve private channel'larda monoton sequence â†’ gap detection mÃ¼mkÃ¼n
   - Binance/Bybit'e gÃ¶re daha iyi (onlar sadece orderbook'ta sequence veriyor)
   - MM desync problemini %100 Ã§Ã¶zer âœ…

2. **Snapshot on Subscribe (Section 5.5)**
   - REST API dependency kaldÄ±rÄ±lmÄ±ÅŸ (Binance/OKX'te zorunlu)
   - Race condition ortadan kalkmÄ±ÅŸ
   - Reconnection sÃ¼resi ~2s'ye dÃ¼ÅŸmÃ¼ÅŸ âœ…

3. **In-Memory Orderbook Reconstruction (Section 7.3.4)**
   - Delta'lardan orderbook build â†’ snapshot latency 0ms
   - External service dependency yok
   - Memory efficient (sadece subscribe olan market'ler iÃ§in) âœ…

4. **Sticky Routing (Section 7.4, Opsiyon 3)**
   - User â†’ Pod mapping consistent â†’ private channel routing basit
   - Pod-to-pod relay yerine direct delivery â†’ latency azalÄ±r
   - KrakenD consistent hashing ile â†’ basit implementasyon âœ…

5. **Backpressure + Slow Consumer Handling (Section 7.3.5)**
   - Write channel buffer + drop counter â†’ slow client tespit edilir
   - Warning + graceful disconnect â†’ client'a bildirim gider
   - v1'in "default: drop" silent fail problemi Ã§Ã¶zÃ¼lmÃ¼ÅŸ âœ…

6. **Graceful Disconnect Protocol (Section 5.1.4)**
   - Close code + `retryAfterMs` â†’ client neden disconnect olduÄŸunu biliyor
   - Maintenance window'da client'lar random interval'de reconnect eder (storm prevention)
   - Global exchange'lerde yok (Binance/Bybit/OKX) â†’ Paribu competitive advantage âœ…

---

#### 2.2.2 âš ï¸ Ä°yileÅŸtirilebilir / Eksik Detaylar

##### **M1 â€” Kafka Partition Strategy (Section 7.4)**

**Problem:** PRD'de topic partition sayÄ±sÄ± "Market sayÄ±sÄ± ile orantÄ±lÄ± (1:1 veya N:1)" belirsiz.

**Ã–nerilen yaklaÅŸÄ±m:**
```yaml
# Public topics (broadcast pattern)
ws.ticker:
  partitions: 64  # Market count'tan baÄŸÄ±msÄ±z, load distribution iÃ§in
  key: market     # AynÄ± market aynÄ± partition'a dÃ¼ÅŸer (ordering guarantee)
  
ws.orderbook.delta:
  partitions: 64
  key: market

ws.trades:
  partitions: 64
  key: market

# Private topics (user-keyed)
ws.orders:
  partitions: 64
  key: user_id    # Sticky routing ile sync

ws.fills:
  partitions: 64
  key: user_id

ws.balances:
  partitions: 64
  key: user_id
```

**Rationale:**
- 64 partition â†’ yeterli parallelism, ama overhead az
- Market-based partitioning â†’ aynÄ± market'in message ordering guarantee
- User-based partitioning (private) â†’ sticky routing ile match eder

**Action Item:**
```
[ ] PRD Section 7.4'e partition strategy ekle (partition count + key strategy)
```

---

##### **M2 â€” Sequence Number Overflow Handling (Section 5.4)**

**PRD'de belirtilen:** "2^53 (JavaScript safe integer). Pratikte overflow olmaz (~285 milyon yÄ±l @ 1M msg/s)."

**Problem:** Bu hesaplama **single channel** iÃ§in. Ama her pod kendi sequence counter'Ä±nÄ± tutuyorsa:
- Pod restart â†’ sequence 1'den baÅŸlar
- Client'Ä±n `lastSeq` buffer'Ä±nda eski sequence var (Ã¶rneÄŸin 1M) â†’ yeni pod'dan 1 gelirse gap detection false positive

**Ã‡Ã¶zÃ¼m alternatifleri:**
1. **Global sequence (Kafka offset):** Kafka partition offset doÄŸrudan sequence olarak kullan
   - âœ… Pod-independent, persistent
   - âœ… Overflow riski yok (Kafka offset 64-bit)
   - âŒ Consumer offset commit delay varsa sequence tutarsÄ±zlÄ±ÄŸÄ± olabilir

2. **Pod-local sequence + connection metadata:** Client reconnect'te yeni pod'a baÄŸlanÄ±rsa, server `lastSeq` buffer'Ä± kabul etmez (farklÄ± pod) â†’ full snapshot gÃ¶nderir
   - âœ… Basit implementasyon
   - âŒ Pod restart'ta tÃ¼m client'lar snapshot alÄ±r (bandwidth spike)

3. **Distributed sequence (Redis atomic counter):** Redis'te per-channel counter
   - âœ… Pod-independent
   - âŒ Redis dependency, latency ekler

**Ã–nerilen:** **Opsiyon 1 (Kafka offset as sequence)**
- Public channel'larda zaten uygulanabilir (Kafka message offset doÄŸrudan sequence)
- Private channel'larda: user-specific sequence gerekiyor, bu durumda Opsiyon 2 (pod-local + snapshot fallback) kabul edilebilir

**Action Item:**
```
[ ] PRD Section 5.4 ve 7.3.3'e sequence source strategy ekle
[ ] Public: Kafka offset = sequence
[ ] Private: Pod-local sequence, reconnect farklÄ± pod'a dÃ¼ÅŸerse full snapshot
```

---

##### **M3 â€” Connection ID Format ve Collision Risk (Section 5.1.1)**

**PRD'de:** `connId: "c_abc123def"` â€” format ve generation aÃ§Ä±klanmamÄ±ÅŸ.

**Problem:** Multi-pod ortamda collision riski.

**Ã–nerilen format:**
```
c_{pod_id}_{timestamp_ns}_{random_6char}
Ã–rnek: c_pod3_1740000000123456_a7f9e2
```

**AvantajlarÄ±:**
- Pod ID â†’ hangi pod'da oluÅŸtuÄŸu belli (debug iÃ§in)
- Timestamp â†’ oluÅŸturulma zamanÄ± (log correlation)
- Random suffix â†’ collision prevention (aynÄ± nanosecond'te iki connection)

**Action Item:**
```
[ ] PRD Section 5.1.1'e connId format spec ekle
```

---

##### **M4 â€” HPA (Horizontal Pod Autoscaler) Metrik SeÃ§imi (Section 7.5)**

**PRD'de:** "Metric: Active WebSocket connections per pod"

**Problem:** Connection count her zaman doÄŸru metrik deÄŸil:
- 1000 connection ama hiÃ§ message gÃ¶nderilmiyorsa (idle client'lar) â†’ CPU dÃ¼ÅŸÃ¼k
- 500 connection ama hepsi orderbook subscribe (high throughput) â†’ CPU yÃ¼ksek

**Ã–nerilen HPA metrik kombinasyonu:**
```yaml
metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70  # Primary metric
  
  - type: Pods
    pods:
      metric:
        name: ws_connections_active
      target:
        type: AverageValue
        averageValue: 8000      # Secondary (connection count)
  
  - type: Pods
    pods:
      metric:
        name: ws_messages_sent_rate
      target:
        type: AverageValue
        averageValue: 400000    # Tertiary (throughput)
```

**HPA policy:**
- Scale up: CPU >70% VEYA connection >8K VEYA message rate >400K/s â†’ 2 dakika
- Scale down: CPU <40% VE connection <4K VE message rate <200K/s â†’ 10 dakika (slow)

**Action Item:**
```
[ ] PRD Section 7.5 HPA metrik stratejisini zenginleÅŸtir (CPU + connection + throughput)
```

---

##### **M5 â€” Snapshot Cache Staleness (Section 7.3.4)**

**PRD'de:** "Cache invalidation: Her delta mesajÄ±nda cache gÃ¼ncellenir"

**Problem:** Multi-pod scenario'da:
- Pod A: Orderbook cache gÃ¼ncelledi (delta seq=1000 aldÄ±)
- Pod B: HenÃ¼z delta seq=1000 consume etmedi (consumer lag)
- Client pod B'ye subscribe ediyor â†’ stale snapshot alÄ±r (seq=999)
- Sonra delta seq=1000 gelir â†’ client duplicate veya gap sanÄ±r

**Ã‡Ã¶zÃ¼m:**
- Snapshot'ta her zaman **timestamp** ve **lastUpdateId** (match engine global sequence) olmalÄ±
- Client snapshot aldÄ±ktan sonra gelen delta'nÄ±n `seq`'ini deÄŸil, `lastUpdateId`'sini karÅŸÄ±laÅŸtÄ±rmalÄ±
- Veya: Snapshot gÃ¶nderildiÄŸinde, o anki pod'un consume ettiÄŸi **son delta seq** dahil edilmeli

**Ã–nerilen snapshot payload:**
```json
{
  "ch": "orderbook@btc_tl",
  "type": "snapshot",
  "seq": 10000,           // Pod'un bu channel'daki son seq
  "snapshotSeq": 10000,   // Bu snapshot'Ä±n dayandÄ±ÄŸÄ± seq
  "lastUpdateId": 9928341, // Match engine global seq
  "ts": 1740000000000,
  "data": { ... }
}
```

Client mantÄ±ÄŸÄ±:
1. Snapshot `snapshotSeq=10000` aldÄ±
2. Sonraki delta `seq=10001` gelmeli
3. `seq != snapshotSeq + 1` ise â†’ gap, resubscribe

**Action Item:**
```
[ ] PRD Section 7.3.4 ve 5.2.1'e snapshot metadata ekle (snapshotSeq, lastUpdateId)
[ ] Client SDK'da gap detection mantÄ±ÄŸÄ± bu metadata'ya gÃ¶re implement edilmeli
```

---

##### **M6 â€” JWT Expiry ve Long-Lived Connection (Edge Case E5)**

**PRD'de:** "JWT 24 saat sonra expire olur, connection hala aktif. Connection korunur."

**Problem:** Security perspective'den sorunlu:
- User token revoke edilmiÅŸ (Ã¶rneÄŸin account compromised) ama connection 24 saat aÃ§Ä±k kalÄ±r
- JWT blacklist (Redis) var ama sadece yeni connection'larda kontrol ediliyor

**Ã–nerilen Ã§Ã¶zÃ¼m:**
1. **Periodic JWT revalidation:** ws-hub her 1 saatte bir active connection'larÄ±n JWT'sini KrakenD Ã¼zerinden validate eder (veya Redis blacklist check)
   - Revoke edilmiÅŸse â†’ graceful disconnect (code 4003)
   
2. **KrakenD push notification:** JWT revoke edildiÄŸinde KrakenD, ilgili user'Ä±n connection'larÄ±na disconnect mesajÄ± gÃ¶nderir
   - ws-hub internal API: `POST /admin/disconnect?userId=u_12345&reason=token_revoked`

**Ã–nerilen:** **Opsiyon 2** (push notification) â€” daha efficient, real-time revoke.

**Action Item:**
```
[ ] PRD Section 9.2'ye token revocation propagation mechanism ekle
[ ] ws-hub admin API: force disconnect endpoint
```

---

##### **M7 â€” Private Channel Routing Sticky Hash Collision (Section 7.4)**

**PRD'de Ã¶nerilen:** "Sticky routing via KrakenD. User her zaman aynÄ± pod'a dÃ¼ÅŸer."

**Problem:** User'Ä±n hangi pod'a map olduÄŸu, **consistent hash ring** ile belirleniyor. Pod scale-up/down olduÄŸunda ring deÄŸiÅŸir â†’ user farklÄ± pod'a dÃ¼ÅŸer.

**Senaryo:**
1. User pod-3'e baÄŸlÄ± (hash(userId) % 5 = 3)
2. HPA scale-up: 5 â†’ 10 pod
3. Consistent hash ring deÄŸiÅŸir: hash(userId) % 10 = 7
4. User reconnect ediyor â†’ pod-7'ye dÃ¼ÅŸÃ¼yor âœ…
5. **Ama:** User reconnect etmeden Ã¶nce, pod-3 hala eski baÄŸlantÄ±yÄ± tutuyor
6. Kafka consumer rebalance: user'Ä±n partition'Ä± artÄ±k pod-7'ye assign edildi
7. pod-3'te aÃ§Ä±k connection var ama private message artÄ±k gelmiyor (pod-7 consume ediyor)

**Ã‡Ã¶zÃ¼m alternatifleri:**
1. **Graceful reconnect on scale:** Scale event'te tÃ¼m client'lara `disconnecting` (reason: rebalance) gÃ¶nder
   - âœ… Clean state, user yeni pod'a baÄŸlanÄ±r
   - âŒ Scale her olduÄŸunda tÃ¼m client'lar disconnect (bad UX)

2. **Pod-to-pod relay (fallback):** Partition owner pod, message'Ä± user'Ä±n baÄŸlÄ± olduÄŸu pod'a forward eder
   - âœ… User disconnect olmadan devam eder
   - âŒ Extra hop, latency artÄ±ÅŸÄ±
   - âŒ Complexity (PRD'de rejected, Opsiyon 1)

3. **Lazy connection migration:** Pod-3, private message almayÄ± kestiÄŸinde (1 dakika timeout), client'a `reconnect_required` event gÃ¶nderir
   - âœ… Scale event'te immediate disconnect yok
   - âœ… User eventually yeni pod'a geÃ§er
   - âŒ 1 dakika boyunca message loss (kabul edilebilir mi?)

**Ã–nerilen:** **Opsiyon 1** (graceful reconnect on scale) â€” UX impact var ama consistency guarantee mÃ¼kemmel.

**Ek Ã¶neri:** Scale-down'Ä± **yavaÅŸ yap** (PRD'de "slow scale-down to avoid churn" var âœ…) ve off-peak saatlerde schedule et.

**Action Item:**
```
[ ] PRD Section 7.4'e pod scale event handling ekle
[ ] KrakenD consistent hash ring update event'inde ws-hub'lara notification gÃ¶nder
[ ] ws-hub affected user'larÄ± graceful disconnect etsin
```

---

##### **M8 â€” Gap Buffer Memory Efficiency (Section 7.3.3)**

**PRD'de:** "Buffer boyutu: ~100K mesaj per popular channel. Memory tahmini: ~50MB per pod."

**Problem:** Bu hesaplama single channel iÃ§in. Popular market'lerde (Ã¶rneÄŸin BTC_TL) birden fazla channel:
- `orderbook@btc_tl` â†’ 100K mesaj buffer
- `trades@btc_tl` â†’ 100K mesaj buffer
- `ticker@btc_tl` â†’ 100K mesaj buffer
- Total: 300K mesaj Ã— 500 byte = 150MB (sadece BTC_TL iÃ§in)

**10 popular market Ã— 150MB = 1.5GB** (per pod)

**Ã–nerilen optimizasyon:**
1. **Adaptive buffer size:** DÃ¼ÅŸÃ¼k throughput channel'larda buffer kÃ¼Ã§Ã¼k (Ã¶rneÄŸin ticker: 1K mesaj yeterli)
2. **TTL-based eviction:** 5 dakika eski mesajlar otomatik silinir (time-based deÄŸil message count-based)
3. **Per-channel buffer cap:** Orderbook: 100K, trades: 50K, ticker: 1K (channel type'a gÃ¶re)

**Ã–nerilen buffer policy:**
```go
bufferSizes := map[string]int{
    "orderbook@*":     100_000,  // High-throughput delta stream
    "trades@*":        50_000,
    "orderbook.*@*":   1_000,    // Snapshot channel (throttled)
    "ticker@*":        1_000,
    "bbo@*":           10_000,
    "kline.*@*":       1_000,
    // Private channels (low throughput per user)
    "orders@*":        1_000,
    "fills@*":         1_000,
    "balances":        100,
}
```

**Action Item:**
```
[ ] PRD Section 7.3.3'e per-channel buffer size policy ekle
[ ] Memory footprint hesaplamasÄ±nÄ± revize et (realistic scenario ile)
```

---

#### 2.2.3 ğŸ”´ Kritik Eksik: Disaster Recovery (DR)

**PRD'de:** Section 3 (Non-Goals): "Multi-region failover: Tek region (Ä°stanbul) ile baÅŸlanacak. DR ayrÄ± proje."

**Problem:** v2 production-critical bir sistem (MM'ler 7/24 baÄŸÄ±mlÄ±). Single-region deployment:
- AWS Ä°stanbul region outage â†’ tÃ¼m WS stream down
- Datacenter network issue â†’ market data akÄ±ÅŸÄ± kesilir

**Ã–nerilen minimum DR (Phase 2-3'te):**
1. **Kafka cross-region replication:** MirrorMaker 2.0 ile Ä°stanbul â†’ Frankfurt replication (async)
2. **Passive standby ws-hub:** Frankfurt'ta ws-hub pod'larÄ± passive modda (Kafka consume ediyor ama client kabul etmiyor)
3. **DNS failover:** Route53 health check â†’ Ä°stanbul down ise stream.paribu.com â†’ Frankfurt'a yÃ¶nlendir
4. **Client reconnect:** Existing client'lar disconnect olur, DNS resolve ederek Frankfurt'a baÄŸlanÄ±r (~30 saniye RTO)

**Not:** DR ayrÄ± PRD olabilir ama v2 launch roadmap'inde mention edilmeli (Ã¶rneÄŸin Phase 4-5).

**Action Item:**
```
[ ] PRD Section 17.1'e DR plan sorusu ekle (Q9)
[ ] Section 8 Migration Strategy'ye DR timeline hint ekle
```

---

### 2.3 AÃ§Ä±k Sorular (Q1-Q8) DeÄŸerlendirmesi

PRD Section 17.1'deki 8 aÃ§Ä±k soruya Ã¶neriler:

---

#### **Q1: Private topic routing â€” Sticky hash mÄ±, broadcast mÄ±, internal relay mi?**

**PRD'de Ã¶nerilen:** Sticky hash (Opsiyon 3)

**DeÄŸerlendirme:** âœ… **DoÄŸru seÃ§im**

**Rationale:**
- Basit implementasyon (KrakenD consistent hash built-in)
- User state tek pod'da â†’ sequence management basit
- Kafka partition assignment efficient (her pod kendi user set'ini consume eder)

**Ek Ã¶neri:** Pod scale event'te graceful reconnect mechanism ekle (yukarÄ±da M7'de detaylandÄ±rÄ±ldÄ±)

**Karar:** **ONAYLA â€” Sticky hash kullan** âœ…

---

#### **Q2: Sequence number â€” Kafka offset mi, baÄŸÄ±msÄ±z counter mÄ±?**

**PRD'de:** Belirsiz

**DeÄŸerlendirme:** **Hybrid yaklaÅŸÄ±m Ã¶ner**

**Ã–neri:**
- **Public channel:** Kafka partition offset doÄŸrudan sequence olarak kullan
  - âœ… Pod-independent, persistent
  - âœ… Multi-pod consistency
  - âŒ Kafka internal detail leak ediyor (sequence gap = Kafka rebalance?)
  
- **Private channel:** Pod-local atomic counter
  - âœ… User-specific, Kafka offset ile karÄ±ÅŸtÄ±rÄ±lmaz
  - âŒ Pod restart'ta reset

**Trade-off:** Public channel'da Kafka offset kullanmak teknik olarak "sÄ±zdÄ±rÄ±yor" ama pratik avantajlar bÃ¼yÃ¼k.

**Karar:** **ONAYLA â€” Public: Kafka offset, Private: Pod-local counter** âœ…

---

#### **Q3: API key authentication â€” v2 GA'da mÄ±, sonra mÄ±?**

**PRD'de:** Belirsiz (Section 9.2'de mention var ama timeline yok)

**DeÄŸerlendirme:** **Phase 2 (MM Beta) sonrasÄ±nda ekle**

**Rationale:**
- MM'ler JWT refresh yÃ¶netiminden ÅŸikayetÃ§i (bot'lar iÃ§in JWT expire handling zor)
- API key static â†’ bot reconnect mantÄ±ÄŸÄ± basitleÅŸir
- KrakenD'de API key validation JWT kadar kolay (header check + Redis lookup)

**Implementation:**
```
Authorization: ApiKey <key>
```
KrakenD API key'i Redis'te validate eder (user_id mapping), `X-User-Id` header'Ä± ile ws-hub'a iletir.

**Timeline:**
- v2 GA (Phase 3) â†’ JWT ile launch
- Phase 3 sonrasÄ± (week 8-12) â†’ API key support ekle
- MM feedback: JWT yeterli mi yoksa API key gerekli mi? â†’ karar

**Karar:** **Phase 2 MM Beta feedback'ine gÃ¶re karar ver** â³

---

#### **Q4: `positions` channel â€” ne zaman aktifleÅŸecek?**

**PRD'de:** "Futures PRD'ye baÄŸlÄ±"

**DeÄŸerlendirme:** âœ… **DoÄŸru yaklaÅŸÄ±m â€” Placeholder olarak bÄ±rak**

**Ã–neri:** Futures Ã¼rÃ¼nÃ¼ launch edilmeden `positions` channel **disabled** olsun. Client subscribe ederse error:
```json
{"id": 1, "code": 40002, "msg": "channel not available yet"}
```

**Karar:** **ONAYLA â€” Futures launch'a kadar placeholder** âœ…

---

#### **Q5: SDK dilleri â€” Python + TypeScript yeterli mi?**

**PRD'de:** Belirsiz

**DeÄŸerlendirme:** **Python + TypeScript + Go Ã¶ncelikli**

**Rationale:**
- **Python:** MM'lerin %60'Ä± Python kullanÄ±yor (quant/algo trading)
- **TypeScript:** Web frontend + Node.js bot'lar
- **Go:** Backend bot'lar (low-latency, high-throughput)
- Java/C#: Talep gelirse Phase 3-4'te ekle

**SDK feature set:**
- Connection management (reconnect, backoff)
- Subscription management (subscribe/unsubscribe)
- Sequence gap detection + auto-resubscribe
- Orderbook reconstruction (delta â†’ full state)
- Rate limit handling (429 error â†’ backoff)

**Timeline:**
- v2 GA launch â†’ Python + TypeScript SDK
- Phase 3 (week 8-12) â†’ Go SDK (MM feedback'ine gÃ¶re)

**Karar:** **Python + TypeScript GA'da, Go Phase 3'te** âœ…

---

#### **Q6: MM'ler iÃ§in dedicated pod pool gerekli mi?**

**PRD'de:** "Phase 2'de deÄŸerlendirilecek"

**DeÄŸerlendirme:** **Phase 2 MM Beta'da karar ver**

**Senaryolar:**
1. **Shared pod pool (default):** MM ve retail aynÄ± pod'larda
   - âœ… Kaynak efficient
   - âŒ Retail traffic spike â†’ MM latency artar
   
2. **Dedicated MM pod pool:** `ws-hub-v2-mm` deployment (ayrÄ± pod set)
   - âœ… MM traffic izole, latency guarantee
   - âŒ Resource overhead (idle capacity)

**Test:** Phase 2 beta'da shared pool ile baÅŸla. MM'ler latency problemi yaÅŸarsa dedicated pool ekle.

**Implementation (dedicated pool):**
```yaml
# KrakenD routing
/v2?tier=mm â†’ ws-hub-v2-mm (3 pod, guaranteed capacity)
/v2 â†’ ws-hub-v2 (10 pod, retail)
```

**Karar:** **Shared pool ile baÅŸla, MM beta feedback'ine gÃ¶re karar** â³

---

#### **Q7: Message batching (1ms window) â€” default aÃ§Ä±k mÄ± kapalÄ± mÄ±?**

**PRD'de:** "Optional, configurable"

**DeÄŸerlendirme:** **Default KAPALI, MM tier iÃ§in aÃ§Ä±labilir**

**Rationale:**
- Batching: Throughput artÄ±rÄ±r ama latency ekler (+1ms avg)
- MM'ler: Latency-sensitive â†’ batching istemezler
- Retail: Throughput-sensitive â†’ batching kabul edilebilir

**Ã–nerilen config:**
```yaml
# ws-hub v2 config
writeBatching:
  enabled: false           # Default kapalÄ±
  windowMs: 1              # Batch window
  enableForTier: ["retail"] # Sadece retail tier'da aÃ§
```

MM connection'larda batching disabled, retail'de enabled.

**Karar:** **Default kapalÄ±, tier-based toggle** âœ…

---

#### **Q8: KrakenD WebSocket proxy â€” yeterli mi yoksa custom Go proxy mi?**

**PRD'de:** "PoC gerekli"

**DeÄŸerlendirme:** **PoC yap, karar Phase 0'da verilmeli**

**KrakenD WebSocket proxy sÄ±nÄ±rlamalarÄ±:**
- Header injection (JWT â†’ `X-User-Id`) â†’ mÃ¼mkÃ¼n âœ…
- Sticky routing (consistent hash) â†’ mÃ¼mkÃ¼n âœ…
- Connection limit per IP â†’ mÃ¼mkÃ¼n âœ…
- **Ama:** KrakenD CPU overhead (TLS termination + routing) â†’ 10K conn/pod handle edebilir mi?

**PoC test:**
- KrakenD 1 pod â†’ 10K concurrent WS connection (wscat veya k6)
- CPU/memory profiling
- Latency (TLS handshake, message forwarding)

**Alternatif (custom Go proxy):**
- `fasthttp` + `gorilla/websocket` veya `gobwas/ws`
- TLS termination + JWT validation + sticky routing
- âœ… Daha lightweight
- âŒ Development effort (2-3 hafta)

**Karar noktasÄ±:** KrakenD PoC'de 10K conn/pod @ <5ms latency saÄŸlarsa â†’ KrakenD kullan. DeÄŸilse custom proxy.

**Karar:** **PoC sonucuna gÃ¶re (Phase 0'da test et)** â³

---

### 2.4 Risk DeÄŸerlendirmesi (R1-R8)

PRD Section 17.2'deki 8 riski analiz:

---

#### **R1: KrakenD WebSocket proxy performans bottleneck**

**PRD deÄŸerlendirmesi:**
- OlasÄ±lÄ±k: Orta
- Etki: YÃ¼ksek
- Mitigation: PoC ile erken test

**Review deÄŸerlendirmesi:** âœ… **Yeterli mitigation**

**Ek Ã¶neri:**
- PoC'de test metrikleri: CPU, memory, connection handling capacity, latency p99
- EÄŸer PoC fail ederse: Custom Go proxy 2-3 hafta development gerektirir â†’ timeline'a buffer ekle

**Mitigation yeterliliÄŸi:** âœ… **YETER** (PoC condition'Ä± var)

---

#### **R2: In-memory orderbook reconstruction memory pressure**

**PRD deÄŸerlendirmesi:**
- OlasÄ±lÄ±k: DÃ¼ÅŸÃ¼k
- Etki: Orta
- Mitigation: Market baÅŸÄ±na OB boyut limiti, lazy load

**Review deÄŸerlendirmesi:** âš ï¸ **Mitigation gÃ¼Ã§lendirilmeli**

**Hesaplama:**
- 500 market Ã— 10,000 level orderbook Ã— 100 byte/level = **500MB per pod** (tÃ¼m market'ler subscribe edilirse)
- GerÃ§ekte: Sadece subscribe olan market'ler â†’ ~50 popular market Ã— 10K level = 50MB âœ…

**Ancak:** Edge case kontrolÃ¼ eksik:
1. **OOM killer scenario:** Memory limit 2GB, OB 500MB, gap buffer 1.5GB â†’ OOM risk var
2. **Market explosion:** 500 market â†’ 2000 market (futures eklenirse)

**Ek mitigation Ã¶nerileri:**
1. **Per-market OB level cap:** Max 10K level (daha derin orderbook subscribe edilemez)
2. **Memory alert:** Pod memory >1.5GB â†’ alert, yeni subscribe request reject
3. **Graceful degradation:** Memory pressure'da eski market'lerin OB cache'i evict edilir (LRU)

**Mitigation yeterliliÄŸi:** âš ï¸ **Ä°YÄ°LEÅTÄ°RÄ°LEBÄ°LÄ°R** (yukarÄ±daki 3 Ã¶neri eklensin)

---

#### **R3: Sticky routing ile pod restart'ta connection storm**

**PRD deÄŸerlendirmesi:**
- OlasÄ±lÄ±k: Orta
- Etki: Orta
- Mitigation: Graceful shutdown + staggered reconnect (retryAfterMs randomization)

**Review deÄŸerlendirmesi:** âœ… **Yeterli mitigation**

**Ek doÄŸrulama:** PRD Section 5.1.4'te `retryAfterMs` var, client SDK'da randomization eklenmeli:
```python
retry_after_ms = server_retry_after_ms + random.randint(0, 5000)
```

**Mitigation yeterliliÄŸi:** âœ… **YETER**

---

#### **R4: Kafka Redpanda topic sayÄ±sÄ± artÄ±ÅŸÄ±**

**PRD deÄŸerlendirmesi:**
- OlasÄ±lÄ±k: DÃ¼ÅŸÃ¼k
- Etki: DÃ¼ÅŸÃ¼k
- Mitigation: 500 market Ã— 6 topic = 3000 partition manageable

**Review deÄŸerlendirmesi:** âœ… **Yeterli**

**Ek bilgi:** Redpanda 10K+ partition handle edebilir (Kafka'dan daha iyi). 3000 partition dÃ¼ÅŸÃ¼k.

**Mitigation yeterliliÄŸi:** âœ… **YETER**

---

#### **R5: MM'lerin v2'ye geÃ§mek istememesi**

**PRD deÄŸerlendirmesi:**
- OlasÄ±lÄ±k: Orta
- Etki: YÃ¼ksek
- Mitigation: Beta'da MM'lerle yakÄ±n Ã§alÄ±ÅŸma, v1 sunset timeline aÃ§Ä±k

**Review deÄŸerlendirmesi:** âš ï¸ **Mitigation gÃ¼Ã§lendirilmeli**

**Ek Ã¶neri:**
1. **Value proposition document:** MM'lere Ã¶zel PDF (sequence number â†’ desync sÄ±fÄ±r, snapshot â†’ reconnect hÄ±zlÄ±, vb.)
2. **Migration incentive:** v2'ye geÃ§en ilk 5 MM'e trading fee discount (%10, 3 ay)
3. **1:1 onboarding support:** Her MM iÃ§in dedicated engineer (1 hafta)
4. **v1 deprecation agressive timeline:** 6 ay deÄŸil 4 ay (urgency yaratÄ±r)

**Mitigation yeterliliÄŸi:** âš ï¸ **Ä°YÄ°LEÅTÄ°RÄ°LEBÄ°LÄ°R** (incentive eklensin)

---

#### **R6: Sequence gap buffer memory (5 dk Ã— yÃ¼ksek throughput market)**

**PRD deÄŸerlendirmesi:**
- OlasÄ±lÄ±k: DÃ¼ÅŸÃ¼k
- Etki: Orta
- Mitigation: Per-channel buffer size cap, oldest-first eviction

**Review deÄŸerlendirmesi:** âœ… **Yeterli** (yukarÄ±da M8'de detaylandÄ±rÄ±ldÄ±)

**Mitigation yeterliliÄŸi:** âœ… **YETER**

---

#### **R7: Gorilla WebSocket library maintenance durumu**

**PRD deÄŸerlendirmesi:**
- OlasÄ±lÄ±k: DÃ¼ÅŸÃ¼k
- Etki: DÃ¼ÅŸÃ¼k
- Mitigation: `nhooyr.io/websocket` veya `gobwas/ws` alternatif

**Review deÄŸerlendirmesi:** âœ… **Yeterli**

**Ek bilgi:** `gorilla/websocket` hala maintained (son commit 2024). Ancak `nhooyr.io/websocket` daha modern (context support, better performance).

**Ã–neri:** **`nhooyr.io/websocket` kullan** (Gorilla deprecated deÄŸil ama nhooyr daha iyi)

**Mitigation yeterliliÄŸi:** âœ… **YETER**

---

#### **R8: Multi-pod private channel routing complexity**

**PRD deÄŸerlendirmesi:**
- OlasÄ±lÄ±k: Orta
- Etki: YÃ¼ksek
- Mitigation: Sticky routing ile basitleÅŸtir, PoC ile validate

**Review deÄŸerlendirmesi:** âœ… **Yeterli** (yukarÄ±da M7'de detaylandÄ±rÄ±ldÄ±)

**PoC kapsamÄ±:**
- KrakenD consistent hash test (3 pod â†’ 5 pod scale, user redistribution)
- Kafka consumer rebalance test (partition reassignment)
- Edge case: User baÄŸlÄ±yken pod scale up/down

**Mitigation yeterliliÄŸi:** âœ… **YETER** (PoC ÅŸartÄ± var)

---

### 2.5 Eksik Konular

PRD'de kapsam dÄ±ÅŸÄ± veya eksik bÄ±rakÄ±lan konular:

---

#### **E1: Rate Limiting Enforcement KatmanÄ± (Netlik Eksik)**

**PRD'de:** Section 13 rate limit tanÄ±mlarÄ± var ama **nerede enforce ediliyor** aÃ§Ä±k deÄŸil.

**Sorular:**
- `subscribe` 10 req/s â†’ ws-hub'da mÄ± yoksa KrakenD'de mi kontrol ediliyor?
- Per-connection rate limit â†’ ws-hub connection manager'da counter mÄ± var?
- Per-user total channel limit (1000 max) â†’ ws-hub'da global state mi gerekiyor (Redis)?

**Ã–nerilen yaklaÅŸÄ±m:**
```
Rate Limit KatmanlarÄ±:
1. KrakenD (L7 gateway):
   - Per-IP connection rate (5 conn/s)
   - Per-user connection rate (10 conn/s)
   
2. ws-hub (application level):
   - Per-connection subscribe/unsubscribe/ping rate (in-memory counter)
   - Per-user total channel count (local pod state, best-effort)
   
3. Enforcement:
   - Soft limit: Error response
   - Hard limit (10x in 30s): Graceful disconnect
```

**Action Item:**
```
[ ] PRD Section 13'e rate limit enforcement architecture ekle (hangi katman hangi limiti kontrol ediyor)
```

---

#### **E2: Monitoring Alert Thresholds (Section 10.4)**

**PRD'de:** Alert condition'larÄ± var ama **threshold deÄŸerleri bazÄ±larÄ±nda belirsiz**.

**Ã–rnek:**
- "Connection error spike: >100 errors/min" âœ… Net
- "Message drop rate: >1% of total messages" âš ï¸ Ne kadar sÃ¼re boyunca? 1 dakika mÄ± 10 dakika mÄ±?
- "Kafka consumer lag: >1000 messages for >30s" âœ… Net

**Ã–nerilen alert spec format:**
```yaml
- name: message_drop_rate_high
  condition: ws_messages_dropped_total / ws_messages_sent_total > 0.01
  for: 5m           # Duration
  severity: warning
  action: Page on-call engineer
```

**Action Item:**
```
[ ] PRD Section 10.4'teki tÃ¼m alert'lere `for: <duration>` ekle
```

---

#### **E3: Client SDK Error Handling (Best Practices Eksik)**

**PRD'de:** SDK Python/TypeScript'te implement edilecek ama **error handling best practices** yok.

**Ã–nerilen SDK error handling guide:**

1. **Connection error:**
   - Immediate retry: 1 attempt
   - Exponential backoff: 1s, 2s, 4s, 8s, max 30s
   - Max retry: Infinite (until user stops)

2. **Sequence gap:**
   - Auto-resubscribe (unsubscribe + subscribe)
   - Max 3 attempt, sonra error throw

3. **Rate limit (4029):**
   - Parse `retryAfterMs` from disconnect message
   - Wait `retryAfterMs + jitter` before reconnect

4. **Auth error (4003):**
   - Token refresh (user-provided callback)
   - 1 retry after refresh
   - If still fails â†’ throw error (user action gerekli)

**Action Item:**
```
[ ] PRD'ye Appendix D: SDK Error Handling Best Practices ekle
[ ] SDK repo'da bu best practices implement et
```

---

#### **E4: Backward Compatibility v1 â†’ v2 (Client Migration Script)**

**PRD'de:** Section 8.3'te v1 channel â†’ v2 channel mapping var ama **client migration tool/script** yok.

**Ã–nerilen:** Migration helper script (Python)

```python
# v1_to_v2_migration.py
v1_to_v2_channel_map = {
    "ticker-extended": "ticker@{market}",
    "orderbook": "orderbook@{market}",
    "latest-matches": "trades@{market}",
    "api-orderbook": "orderbook.20@{market}",
    # ...
}

def migrate_subscription(v1_channel, market):
    if v1_channel not in v1_to_v2_channel_map:
        raise ValueError(f"Unknown v1 channel: {v1_channel}")
    
    v2_template = v1_to_v2_channel_map[v1_channel]
    return v2_template.replace("{market}", market)

# Example
v1_channels = ["ticker-extended", "orderbook", "latest-matches"]
market = "btc_tl"
v2_channels = [migrate_subscription(ch, market) for ch in v1_channels]
print(v2_channels)
# Output: ['ticker@btc_tl', 'orderbook@btc_tl', 'trades@btc_tl']
```

**Action Item:**
```
[ ] Migration script'i SDK'ya dahil et (v1-to-v2-migrator)
[ ] docs.paribu.com'da migration guide publish et
```

---

#### **E5: Load Test Scenario Details (Section 14.3)**

**PRD'de:** "Load test" hedefleri var ama **test senaryolarÄ± detaylandÄ±rÄ±lmamÄ±ÅŸ**.

**Ã–nerilen load test scenario spec:**

**Scenario 1: Peak Trading Hour**
```
- 50K concurrent connections
- %60 retail (30K) â†’ 5 channel avg â†’ 150K subscriptions
- %30 MM (15K) â†’ 50 channel avg â†’ 750K subscriptions
- %10 idle (5K) â†’ 0 message
- Message rate: 500K msg/s outbound (broadcast)
- Duration: 2 hours
- Success criteria: p99 latency <25ms, drop rate <0.01%
```

**Scenario 2: Connection Storm (Market Event)**
```
- 10K connection/s spike (30 saniye boyunca)
- Her connection: 10 channel subscribe
- Success criteria: Connection accept rate >90%, latency <50ms
```

**Scenario 3: Kafka Consumer Lag Recovery**
```
- Kill 1 Kafka broker (3-node cluster)
- Consumer lag 10K mesaj'a Ã§Ä±kar
- Success criteria: Lag recovery <5 dakika, zero message loss
```

**Action Item:**
```
[ ] PRD Section 14.3'e load test scenario spec'leri ekle
[ ] k6 + custom Go script ile implement et
```

---

#### **E6: Documentation Checklist (Eksik)**

**PRD'de:** "Documentation publish" mention var ama **kapsamÄ± belirsiz**.

**Ã–nerilen doc structure:**
```
docs.paribu.com/api/v2/websocket/
â”œâ”€â”€ getting-started.md       # Quick start (5 dakikada baÄŸlan)
â”œâ”€â”€ authentication.md        # JWT vs API key
â”œâ”€â”€ channels/
â”‚   â”œâ”€â”€ public.md            # Public channel spec
â”‚   â”œâ”€â”€ private.md           # Private channel spec
â”‚   â””â”€â”€ channel-list.md      # TÃ¼m channel referans tablosu
â”œâ”€â”€ protocol.md              # Frame format, sequence, snapshot
â”œâ”€â”€ error-handling.md        # Error codes, reconnection
â”œâ”€â”€ rate-limits.md           # Rate limit policy
â”œâ”€â”€ sdk/
â”‚   â”œâ”€â”€ python.md            # Python SDK guide
â”‚   â”œâ”€â”€ typescript.md        # TypeScript SDK guide
â”‚   â””â”€â”€ go.md                # Go SDK guide (Phase 3)
â”œâ”€â”€ migration-from-v1.md     # v1 â†’ v2 migration guide
â””â”€â”€ faq.md                   # SÄ±k sorulan sorular
```

**Action Item:**
```
[ ] Documentation outline'Ä± PRD'ye ekle (Appendix E)
[ ] Technical writer assign et (Phase 2)
```

---

### 2.6 Deployment Ä°zolasyon Validasyonu

PRD Section 15 (Rollout Plan) inceleme:

---

#### **D1: Phase 0 â€” Infrastructure HazÄ±rlÄ±ÄŸÄ±**

**PRD checklist:**
- [ ] Kafka topic'leri oluÅŸtur
- [ ] Match engine'den yeni topic'lere publish baÅŸlat (mevcut topic'lere paralel)
- [ ] KrakenD `/v2` route config hazÄ±rla (disabled)
- [ ] ws-hub v2 Docker image build pipeline
- [ ] Monitoring dashboard + alerting rules

**Ä°zolasyon validasyonu:**

âœ… **Kafka topic oluÅŸtur** â€” Yeni consumer group `ws-hub-v2` kullanÄ±lacak, mevcut `ws-hub` (v1) etkilenmez.

âš ï¸ **Match engine yeni topic'lere publish** â€” **"Paralel"** kelimesi belirsiz:
- **Yorum 1:** Match engine **aynÄ± topic'lere** hem v1 hem v2 publish ediyor â†’ v1 etkilenmez (sadece consumer count artÄ±yor)
- **Yorum 2:** Match engine **ayrÄ± topic'lere** publish baÅŸlÄ±yor (Ã¶rneÄŸin `ws.v2.ticker`) â†’ v1 hiÃ§ etkilenmez âœ…

**Action:** PRD'de netleÅŸtirilmeli. **Ã–nerilen:** AynÄ± topic'leri kullan (yorum 1), ama match engine'in publish throughput'unu test et.

âœ… **KrakenD `/v2` route disabled** â€” v2 hazÄ±r olana kadar disabled â†’ v1'e sÄ±fÄ±r etki.

âœ… **ws-hub v2 Docker image** â€” AyrÄ± image, ayrÄ± deployment â†’ v1 binary'si deÄŸiÅŸmez.

âœ… **Monitoring dashboard** â€” AyrÄ± dashboard (`WS Stream v2 Overview`) â†’ v1 dashboard'u kirlenmez.

**Genel deÄŸerlendirme:** âœ… **Ä°zolasyon saÄŸlanmÄ±ÅŸ**, "paralel publish" netleÅŸtirilmeli.

---

#### **D2: Phase 1 â€” Internal Beta**

**PRD checklist:**
- [ ] ws-hub v2 deploy (2 pod, staging)
- [ ] KrakenD `/v2` route enable (staging)
- [ ] Internal QA
- [ ] Performance baseline

**Ä°zolasyon validasyonu:**

âœ… **Staging deployment** â€” Production v1'e sÄ±fÄ±r etki (ayrÄ± ortam).

âš ï¸ **KrakenD staging** â€” EÄŸer staging KrakenD, production Kafka'ya baÄŸlanÄ±yorsa â†’ staging v2 test traffic'i production Kafka'ya gider â†’ v1 etkilenebilir.

**Action:** Staging ortamÄ± **tamamen izole** olmalÄ± (staging Kafka kullanmalÄ±).

**Genel deÄŸerlendirme:** âœ… **Ä°zolasyon saÄŸlanmÄ±ÅŸ** (staging isolated ise).

---

#### **D3: Phase 2 â€” MM Beta (Production)**

**PRD checklist:**
- [ ] Production deploy (3 pod, low traffic)
- [ ] KrakenD `/v2` route enable (production, whitelist IP)
- [ ] 2-3 MM partner onboard

**Ä°zolasyon validasyonu:**

âœ… **AyrÄ± pod set (3 pod)** â€” v1 pod'larÄ± (Ã¶rneÄŸin 10 pod) etkilenmez.

âœ… **Whitelist IP** â€” Sadece beta MM'ler baÄŸlanabilir â†’ traffic kontrollÃ¼.

âš ï¸ **KrakenD route enable** â€” Production KrakenD'de `/v2` route aÃ§Ä±lÄ±yor:
- EÄŸer **shared KrakenD instance** kullanÄ±lÄ±yorsa â†’ v2 traffic KrakenD CPU/memory tÃ¼ketir
- Beta traffic dÃ¼ÅŸÃ¼k (2-3 MM, ~10-50 connection) â†’ etki minimal ama monitÃ¶r edilmeli

**Action:** KrakenD CPU/memory monitoring Phase 2 boyunca aktif olmalÄ±. Threshold: v2 traffic, KrakenD CPU'nun >%10'unu tÃ¼ketiyorsa alarm.

âœ… **Kafka consumer group `ws-hub-v2`** â€” AyrÄ± consumer group â†’ v1 etkilenmez.

**Genel deÄŸerlendirme:** âœ… **Ä°zolasyon saÄŸlanmÄ±ÅŸ**, KrakenD monitÃ¶r edilmeli.

---

#### **D4: Phase 3 â€” Public GA**

**PRD checklist:**
- [ ] KrakenD `/v2` route open (all users)
- [ ] Scale to 5-10 pods

**Ä°zolasyon validasyonu:**

âœ… **AyrÄ± route `/v2`** â€” v1 route'u (`/ws`) etkilenmez.

âš ï¸ **Scale to 10 pods** â€” Kubernetes cluster capacity yeterli mi?
- v1: 10 pods
- v2: 10 pods
- Total: 20 pods â†’ node capacity kontrolÃ¼ gerekli

**Action:** Cluster capacity planning â€” node pool'da yeterli kaynak var mÄ±?

âš ï¸ **KrakenD shared instance load** â€” v2 GA'da traffic 100x artabilir (50K connection):
- KrakenD CPU/memory yeterli mi?
- KrakenD HPA var mÄ±? (Ã¶rneÄŸin 3 pod â†’ 6 pod auto-scale)

**Action:** KrakenD HPA policy tanÄ±mla (CPU >70% â†’ scale up).

**Genel deÄŸerlendirme:** âš ï¸ **Ä°zolasyon saÄŸlanmÄ±ÅŸ ama capacity planning critical**.

---

#### **D5: Phase 4 â€” Migration Push (v1 Deprecation)**

**PRD checklist:**
- [ ] v1 deprecation header ekle
- [ ] v1 connection limit kademeli dÃ¼ÅŸÃ¼r

**Ä°zolasyon validasyonu:**

âœ… **v1'e header ekle** â€” Response'lara `X-Deprecation: 2026-08-01` eklemek v2'ye etki etmez.

âœ… **v1 connection limit dÃ¼ÅŸÃ¼r** â€” v1'in pod count'u veya max connection'Ä± azaltmak v2'ye etki etmez.

**Genel deÄŸerlendirme:** âœ… **Ä°zolasyon korunmuÅŸ**.

---

#### **D6: Phase 5 â€” v1 Sunset**

**PRD checklist:**
- [ ] v1 yeni connection kabul etmeyi durdur
- [ ] v1 mevcut connection graceful disconnect
- [ ] v1 pod decommission

**Ä°zolasyon validasyonu:**

âœ… **v1 shutdown** â€” v1 pod'larÄ± silindiÄŸinde v2 etkilenmez (ayrÄ± deployment).

âš ï¸ **Connection storm risk** â€” v1 client'lar disconnect olunca v2'ye reconnect etmeye Ã§alÄ±ÅŸabilir:
- Ani 50K connection artÄ±ÅŸÄ± â†’ v2 HPA tetiklenmeli (pod scale up)
- v2'nin HPA yeterince hÄ±zlÄ± mÄ±? (scale-up 2-3 dakika sÃ¼rer)

**Action:** v1 sunset Ã¶ncesi v2 kapasitesini artÄ±r (Ã¶rneÄŸin 10 pod â†’ 15 pod proactive scale).

**Genel deÄŸerlendirme:** âš ï¸ **Ä°zolasyon saÄŸlanmÄ±ÅŸ, v1 â†’ v2 migration traffic spike planlanmalÄ±**.

---

#### **Deployment Ä°zolasyon Ã–zet Skoru**

| Phase | Ä°zolasyon Durumu | Risk | Action Gerekli |
|---|---|---|---|
| Phase 0 (Infra) | âœ… Ä°yi | DÃ¼ÅŸÃ¼k | Kafka publish strategy netleÅŸtir |
| Phase 1 (Beta) | âœ… Ä°yi | Yok | Staging izole olsun |
| Phase 2 (MM Beta) | âœ… Ä°yi | DÃ¼ÅŸÃ¼k | KrakenD monitÃ¶r et |
| Phase 3 (GA) | âš ï¸ Ä°yi ama capacity risk | Orta | Cluster capacity + KrakenD HPA |
| Phase 4 (Deprecation) | âœ… Ä°yi | Yok | â€” |
| Phase 5 (Sunset) | âš ï¸ Ä°yi ama migration spike | Orta | v2 proactive scale |

**GENEL DEÄERLENDÄ°RME:**
- **Deployment izolasyonu tasarÄ±m olarak saÄŸlam** âœ…
- **Operasyonel riskler var (capacity, KrakenD, migration spike)** âš ï¸
- **Action item'lar tamamlanÄ±rsa izolasyon %95+** âœ…

---

## 3. BÃ¶lÃ¼m 2: gRPC Direct Message Delivery Ã–nerisi

### 3.1 Use Case: Neden gRPC?

#### 3.1.1 Hedef Kitle

**Primary:** Market Maker (MM) bot'larÄ±
- Server-side Ã§alÄ±ÅŸan (AWS EC2, dedicated server, Kubernetes pod)
- Browser constraint'i yok (gRPC-Web gerekmez, native gRPC)
- 7/24 uptime, dÃ¼ÅŸÃ¼k latency kritik
- YÃ¼ksek throughput (1000+ msg/s per connection)

**Secondary:** Internal servisler (Ã¶rneÄŸin risk yÃ¶netim sistemi, monitoring bot'larÄ±)

**Non-target:** Web frontend, mobile app (WebSocket ile devam)

---

#### 3.1.2 Motivasyon: WebSocket'in SÄ±nÄ±rlamalarÄ± (MM Perspektifinden)

| Problem | WebSocket | gRPC Streaming |
|---|---|---|
| **Protocol overhead** | JSON text â†’ parse overhead, bÃ¼yÃ¼k payload | Protobuf binary â†’ %40-60 daha kÃ¼Ã§Ã¼k, zero-copy deserialize |
| **Backpressure** | Application-level (slow consumer drop) | Built-in (gRPC flow control, HTTP/2 window) |
| **Connection management** | Tek TCP connection, tek stream | HTTP/2 multiplexing â†’ single TCP, multiple streams |
| **Schema enforcement** | JSON â†’ runtime validation, type safety yok | Protobuf schema â†’ compile-time type safety |
| **Client library** | Generic WebSocket lib + custom protocol | gRPC client auto-generated (10 dakikada entegrasyon) |
| **Load balancing** | Sticky routing gerekli (connection-level) | HTTP/2 request-level LB (daha efficient) |
| **Debugging** | Binary WS frame â†’ hex dump gerekli | gRPC reflection + grpcurl â†’ kolay debug |

---

#### 3.1.3 Use Case SenaryolarÄ±

**UC1: Low-Latency Orderbook Stream**
- MM bot'u BTC_TL orderbook'u takip ediyor (delta stream)
- WebSocket: JSON parse ~0.5ms (Python) veya ~0.1ms (Go)
- gRPC: Protobuf unmarshal ~0.05ms (Go) â†’ **%50 latency azalmasÄ±**

**UC2: Multi-Market High-Throughput**
- MM 50 market Ã— 6 channel = 300 subscription
- WebSocket: 300 subscription â†’ 300 message fan-out â†’ 300Ã— JSON serialize
- gRPC: 300 subscription â†’ 1 gRPC stream â†’ protobuf batch serialize â†’ **CPU %40 azalÄ±r**

**UC3: Backpressure (Slow Consumer)**
- MM bot yavaÅŸladÄ± (Ã¶rneÄŸin DB write bottleneck)
- WebSocket: Message buffer dolar â†’ drop â†’ gap â†’ resubscribe â†’ snapshot â†’ bandwidth spike
- gRPC: Flow control â†’ server yavaÅŸlar (client ready olana kadar bekler) â†’ **zero message loss**

**UC4: Schema Evolution**
- Yeni bir field eklenecek (Ã¶rneÄŸin `orderbook` message'a `timestamp` field)
- WebSocket JSON: Client'lar field gÃ¶rÃ¼rse parse eder, gÃ¶rmezse yok sayar (loose coupling)
- gRPC Protobuf: Field number'lÄ± versioning â†’ backward/forward compatible â†’ **type-safe evolution**

---

### 3.2 Mimari Entegrasyon

#### 3.2.1 Mimari Åema

```
                                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                 â”‚       Kubernetes Cluster                    â”‚
                                 â”‚                                            â”‚
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   WebSocket      â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
  â”‚  Web     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚  â”‚   KrakenD    â”‚â”€â”€â”€â”€â”€â–ºâ”‚  ws-hub-v2   â”‚   â”‚
  â”‚  Client  â”‚  :443/v2         â”‚  â”‚   Gateway    â”‚ WSS  â”‚  (WebSocket) â”‚   â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
                                 â”‚                               â”‚           â”‚
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   gRPC           â”‚                               â”‚           â”‚
  â”‚  MM Bot  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚           â”‚
  â”‚ (Server) â”‚  :50051          â”‚  â”‚  ws-hub-v2   â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚  â”‚   (gRPC)    â”‚  Internal   Shared:     â”‚
                                 â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  call      - Kafka     â”‚
                                 â”‚         â”‚                    - Seq Mgr   â”‚
                                 â”‚         â”‚                    - Snapshot  â”‚
                                 â”‚    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”                          â”‚
                                 â”‚    â”‚  Kafka   â”‚                          â”‚
                                 â”‚    â”‚ Consumer â”‚                          â”‚
                                 â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚
                                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key points:**
1. **ws-hub-v2 binary:** Hem WebSocket hem gRPC server aynÄ± binary'de
2. **Kafka consumer:** Tek consumer group (`ws-hub-v2`), her iki transport'a mesaj daÄŸÄ±tÄ±r
3. **Sequence manager:** PaylaÅŸÄ±mlÄ± (WebSocket ve gRPC aynÄ± sequence number'larÄ± kullanÄ±r)
4. **Snapshot cache:** PaylaÅŸÄ±mlÄ± (in-memory orderbook reconstruction)
5. **KrakenD bypass:** gRPC endpoint doÄŸrudan pod'lara expose (L4 load balancer yeterli)

---

#### 3.2.2 ws-hub-v2 Ä°Ã§ Mimari (gRPC Eklentisi)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ws-hub-v2 pod                         â”‚
â”‚                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚  â”‚  WebSocket  â”‚      â”‚   gRPC       â”‚                 â”‚
â”‚  â”‚  Server     â”‚      â”‚   Server     â”‚                 â”‚
â”‚  â”‚  :8080      â”‚      â”‚   :50051     â”‚                 â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚         â”‚                    â”‚                         â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚
â”‚                  â”‚                                     â”‚
â”‚           â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚           â”‚   Channel     â”‚â—„â”€â”€â”€â”€â”€â”‚  Kafka     â”‚        â”‚
â”‚           â”‚   Router      â”‚      â”‚  Consumer  â”‚        â”‚
â”‚           â”‚  (Unified)    â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                            â”‚
â”‚                  â”‚                                     â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”                            â”‚
â”‚         â”‚                 â”‚                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚  â”‚  WebSocket  â”‚   â”‚   gRPC      â”‚                    â”‚
â”‚  â”‚  Fan-out    â”‚   â”‚   Stream    â”‚                    â”‚
â”‚  â”‚             â”‚   â”‚   Sender    â”‚                    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚                                                        â”‚
â”‚  Shared:                                              â”‚
â”‚  - Sequence Manager                                   â”‚
â”‚  - Snapshot Cache (in-memory OB)                      â”‚
â”‚  - Metrics Exporter                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Unified Channel Router:**
- Kafka message geldiÄŸinde:
  1. WebSocket subscriber'larÄ±na JSON serialize + fan-out
  2. gRPC subscriber'larÄ±na protobuf serialize + stream send
- Client subscription (WS veya gRPC) â†’ aynÄ± internal data structure'a kaydedilir

---

#### 3.2.3 Transport SeÃ§imi: Client Perspektifi

| Client Tipi | Transport | Rationale |
|---|---|---|
| Web browser | WebSocket | gRPC-Web gerektirir (ekstra overhead) |
| Mobile app (React Native, Flutter) | WebSocket | gRPC mobile support var ama WS daha yaygÄ±n |
| Python bot (server-side) | **gRPC** (opsiyonel WS) | `grpcio` library mature, async support |
| Go bot (server-side) | **gRPC** | Native gRPC support, protobuf code generation |
| Node.js bot (server-side) | gRPC veya WS | `@grpc/grpc-js` veya `ws` â€” ikisi de iyi |
| Java/C# bot | **gRPC** | Enterprise standart, mature library |

**SeÃ§im kriteri:** Server-side bot â†’ gRPC tercih, browser/mobile â†’ WebSocket zorunlu.

---

### 3.3 API TasarÄ±mÄ± (Proto Sketch)

#### 3.3.1 Proto File: `stream_v2.proto`

```protobuf
syntax = "proto3";

package paribu.stream.v2;

import "google/protobuf/timestamp.proto";

// =============================================================================
// Service Definition
// =============================================================================

service StreamService {
  // Subscribe to multiple channels (bidirectional streaming)
  rpc Subscribe(stream ClientMessage) returns (stream ServerMessage);
  
  // Health check (unary, for load balancer)
  rpc Ping(PingRequest) returns (PingResponse);
}

// =============================================================================
// Client â†’ Server Messages
// =============================================================================

message ClientMessage {
  oneof message {
    SubscribeRequest subscribe = 1;
    UnsubscribeRequest unsubscribe = 2;
    PingMessage ping = 3;
  }
}

message SubscribeRequest {
  repeated string channels = 1;  // ["ticker@btc_tl", "orderbook@eth_tl"]
  
  // Optional: Resume with last seen sequence (reconnection)
  map<string, uint64> last_seq = 2;  // {"orderbook@btc_tl": 10050}
}

message UnsubscribeRequest {
  repeated string channels = 1;
}

message PingMessage {
  uint64 client_timestamp_ms = 1;  // Client send timestamp
}

// =============================================================================
// Server â†’ Client Messages
// =============================================================================

message ServerMessage {
  oneof message {
    ConnectedEvent connected = 1;
    SubscribeResponse subscribe_response = 2;
    UnsubscribeResponse unsubscribe_response = 3;
    PongMessage pong = 4;
    DataMessage data = 5;
    ErrorMessage error = 6;
    DisconnectingEvent disconnecting = 7;
  }
}

message ConnectedEvent {
  google.protobuf.Timestamp timestamp = 1;
  string conn_id = 2;
  string server_id = 3;  // Pod ID
}

message SubscribeResponse {
  repeated string subscribed = 1;
  repeated ChannelError errors = 2;  // Partial failure
}

message UnsubscribeResponse {
  repeated string unsubscribed = 1;
}

message PongMessage {
  uint64 client_timestamp_ms = 1;  // Echo from ping
  uint64 server_timestamp_ms = 2;
}

message ErrorMessage {
  uint32 code = 1;
  string message = 2;
}

message DisconnectingEvent {
  uint32 code = 1;
  string reason = 2;
  uint32 retry_after_ms = 3;
}

message ChannelError {
  string channel = 1;
  uint32 code = 2;
  string message = 3;
}

// =============================================================================
// Data Message (Unified for All Channels)
// =============================================================================

message DataMessage {
  string channel = 1;                  // "ticker@btc_tl"
  uint64 seq = 2;                      // Sequence number
  google.protobuf.Timestamp timestamp = 3;
  
  oneof payload {
    TickerData ticker = 10;
    OrderbookData orderbook = 11;
    TradesData trades = 12;
    KlineData kline = 13;
    BBOData bbo = 14;
    OrdersData orders = 15;
    FillsData fills = 16;
    BalanceData balance = 17;
  }
}

// =============================================================================
// Channel Payloads
// =============================================================================

message TickerData {
  string last = 1;
  string bid = 2;
  string ask = 3;
  string high = 4;
  string low = 5;
  string vol = 6;
  string quote_vol = 7;
  string change = 8;
  string open_price = 9;
  uint64 close_time = 10;
  uint32 trade_count = 11;
}

message OrderbookData {
  enum Type {
    SNAPSHOT = 0;
    DELTA = 1;
  }
  Type type = 1;
  repeated PriceLevel bids = 2;
  repeated PriceLevel asks = 3;
  uint64 last_update_id = 4;  // Match engine global seq
}

message PriceLevel {
  string price = 1;
  string amount = 2;
}

message TradesData {
  repeated Trade trades = 1;
}

message Trade {
  string trade_id = 1;
  string price = 2;
  string amount = 3;
  string side = 4;  // "buy" or "sell"
  uint64 timestamp = 5;
}

message KlineData {
  uint64 open_time = 1;
  uint64 close_time = 2;
  string open = 3;
  string high = 4;
  string low = 5;
  string close = 6;
  string vol = 7;
  string quote_vol = 8;
  uint32 trade_count = 9;
  bool closed = 10;
}

message BBOData {
  string bid = 1;
  string bid_qty = 2;
  string ask = 3;
  string ask_qty = 4;
}

message OrdersData {
  string order_id = 1;
  string client_order_id = 2;
  string market = 3;
  string status = 4;  // "new", "partially_filled", "filled", "cancelled"
  string type = 5;    // "limit", "market"
  string side = 6;    // "buy", "sell"
  string price = 7;
  string amount = 8;
  string filled = 9;
  string remaining = 10;
  string avg_price = 11;
  string fee = 12;
  string fee_asset = 13;
  uint64 created_at = 14;
  uint64 updated_at = 15;
}

message FillsData {
  string trade_id = 1;
  string order_id = 2;
  string client_order_id = 3;
  string market = 4;
  string side = 5;
  string price = 6;
  string amount = 7;
  string fee = 8;
  string fee_asset = 9;
  bool is_maker = 10;
  uint64 timestamp = 11;
}

message BalanceData {
  string asset = 1;
  string available = 2;
  string locked = 3;
  string total = 4;
}

// =============================================================================
// Health Check (Unary)
// =============================================================================

message PingRequest {}

message PingResponse {
  string status = 1;  // "ok"
  google.protobuf.Timestamp timestamp = 2;
}
```

---

#### 3.3.2 Proto Design Rationale

**1. Bidirectional Streaming (`stream ClientMessage â†” stream ServerMessage`)**
- Client tek stream aÃ§ar, tÃ¼m subscription'lar bu stream Ã¼zerinden
- WebSocket'teki single connection semantiÄŸi korunur
- HTTP/2 multiplexing ile efficient (aynÄ± TCP connection, multiple gRPC streams kullanÄ±labilir)

**2. `oneof` Union Types**
- `ClientMessage` ve `ServerMessage` union type â†’ single stream'de farklÄ± message type'larÄ±
- Protobuf code generation â†’ type-safe switch-case

**3. Decimal Strings (Price/Amount)**
- Protobuf'ta native decimal type yok
- `string` kullanÄ±mÄ± â†’ precision loss yok (JSON ile aynÄ± yaklaÅŸÄ±m)
- Alternative: `int64` (fixed-point, Ã¶rneÄŸin price Ã— 10^8) â†’ daha efficient ama karmaÅŸÄ±k

**4. Timestamp: `google.protobuf.Timestamp`**
- UTC nanosecond precision
- JSON'da `"ts": 1740000000000` (millisecond) yerine protobuf standard type

**5. Channel-Specific Payload (`oneof payload`)**
- Her channel type'Ä± ayrÄ± message (`TickerData`, `OrderbookData`, vb.)
- Type-safe deserialization
- WebSocket JSON'daki `"data": {...}` generic object yerine

---

#### 3.3.3 Client SDK Ã–rneÄŸi (Python)

```python
import grpc
import stream_v2_pb2 as pb
import stream_v2_pb2_grpc as pb_grpc

# gRPC channel + stub
channel = grpc.insecure_channel('stream.paribu.com:50051')
stub = pb_grpc.StreamServiceStub(channel)

# Metadata (auth)
metadata = [('authorization', 'Bearer <JWT>')]

# Bidirectional stream
def request_generator():
    # Subscribe
    yield pb.ClientMessage(
        subscribe=pb.SubscribeRequest(
            channels=['ticker@btc_tl', 'orderbook@btc_tl', 'orders@btc_tl']
        )
    )
    
    # Ping every 20s (keepalive)
    while True:
        time.sleep(20)
        yield pb.ClientMessage(
            ping=pb.PingMessage(client_timestamp_ms=int(time.time() * 1000))
        )

# Stream consume
responses = stub.Subscribe(request_generator(), metadata=metadata)

for msg in responses:
    if msg.HasField('connected'):
        print(f"Connected: {msg.connected.conn_id}")
    
    elif msg.HasField('data'):
        data = msg.data
        if data.HasField('ticker'):
            print(f"[{data.channel}] seq={data.seq} last={data.ticker.last}")
        
        elif data.HasField('orderbook'):
            ob = data.orderbook
            print(f"[{data.channel}] seq={data.seq} type={ob.type} bids={len(ob.bids)}")
        
        elif data.HasField('orders'):
            order = data.orders
            print(f"[{data.channel}] seq={data.seq} order_id={order.order_id} status={order.status}")
    
    elif msg.HasField('pong'):
        latency = time.time() * 1000 - msg.pong.client_timestamp_ms
        print(f"Pong latency: {latency:.2f}ms")
    
    elif msg.HasField('disconnecting'):
        print(f"Disconnecting: {msg.disconnecting.reason}")
        break
```

**Avantajlar (vs WebSocket Python):**
- Proto file'dan auto-generated code (`stream_v2_pb2.py`) â†’ type hints, autocomplete
- `HasField()` ile type-safe message check
- Protobuf deserialization built-in (JSON parse yok)

---

### 3.4 Deployment Stratejisi

#### 3.4.1 Opsiyon 1: Tek Binary, Ä°ki Port (Ã–nerilen)

**ws-hub-v2 deployment:**
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ws-hub-v2
spec:
  replicas: 10
  template:
    spec:
      containers:
      - name: ws-hub-v2
        image: paribu/ws-hub-v2:latest
        args:
          - --websocket-port=8080
          - --grpc-port=50051
        ports:
          - containerPort: 8080  # WebSocket
            name: websocket
          - containerPort: 50051 # gRPC
            name: grpc
        resources:
          limits:
            cpu: 4
            memory: 4Gi
```

**Service (L4 Load Balancer):**
```yaml
apiVersion: v1
kind: Service
metadata:
  name: ws-hub-v2-grpc
spec:
  type: LoadBalancer
  selector:
    app: ws-hub-v2
  ports:
    - port: 50051
      targetPort: 50051
      name: grpc
```

**WebSocket iÃ§in KrakenD (L7), gRPC iÃ§in direkt L4 LB.**

**Avantajlar:**
- âœ… Tek binary â†’ deployment basit
- âœ… PaylaÅŸÄ±mlÄ± Kafka consumer, sequence manager, snapshot cache â†’ kaynak efficient
- âœ… gRPC client'lar direkt pod'lara baÄŸlanÄ±r (KrakenD overhead yok)

**Dezavantajlar:**
- âŒ WS ve gRPC aynÄ± pod'da â†’ resource contention (WS spike gRPC'yi etkileyebilir)

---

#### 3.4.2 Opsiyon 2: AyrÄ± Deployment (Ä°zolasyon Maksimum)

**ws-hub-v2-ws deployment:**
```yaml
- name: ws-hub-v2-ws
  replicas: 10
  args: [--websocket-port=8080, --grpc-enabled=false]
```

**ws-hub-v2-grpc deployment:**
```yaml
- name: ws-hub-v2-grpc
  replicas: 5
  args: [--grpc-port=50051, --websocket-enabled=false]
```

**Avantajlar:**
- âœ… Tam izolasyon (WS spike gRPC'yi etkilemez)
- âœ… Independent scaling (WS 10 pod, gRPC 5 pod)

**Dezavantajlar:**
- âŒ Ä°ki ayrÄ± Kafka consumer group â†’ Kafka partition replication artÄ±ÅŸÄ±
- âŒ Ä°ki ayrÄ± snapshot cache â†’ memory overhead
- âŒ Deployment complexity

---

#### 3.4.3 Ã–nerilen: **Opsiyon 1** (Tek Binary, Ä°ki Port)

**Rationale:**
- MM traffic dÃ¼ÅŸÃ¼k (5-10K connection vs 50K WS connection) â†’ resource contention riski minimal
- PaylaÅŸÄ±mlÄ± state â†’ consistency kolay (sequence sync)
- Basit deployment â†’ operational overhead az

**Mitigation:** Resource contention riski iÃ§in:
- gRPC connection'lara CPU quota Ã¶nceliÄŸi ver (Ã¶rneÄŸin gRPC goroutine'lere nice value)
- WS traffic spike'Ä±nda gRPC latency monitÃ¶r et â†’ eÄŸer >10ms ise Opsiyon 2'ye geÃ§ (Phase 3'te)

---

### 3.5 Authentication

#### 3.5.1 mTLS (Mutual TLS)

**Use case:** Server-to-server (MM bot AWS'de, Paribu Kubernetes cluster ile gÃ¼venli kanal)

**Setup:**
1. Paribu CA certificate MM'e verilir
2. MM client certificate generate eder (Paribu CA imzalÄ±)
3. gRPC connection mTLS ile kurulur

**Go client Ã¶rneÄŸi:**
```go
creds, err := credentials.NewClientTLSFromFile("ca.crt", "")
conn, err := grpc.Dial(
    "stream.paribu.com:50051",
    grpc.WithTransportCredentials(creds),
    grpc.WithPerRPCCredentials(&tlsCreds{cert: clientCert}),
)
```

**Avantajlar:**
- âœ… Token expiry yok (certificate 1 yÄ±l geÃ§erli)
- âœ… Bot restart'ta token refresh gerekmez
- âœ… Network-level security (man-in-the-middle attack impossible)

**Dezavantajlar:**
- âŒ Certificate management overhead (MM'ler certificate renew etmeli)
- âŒ Revocation: Certificate revoke iÃ§in CRL veya OCSP gerekli

---

#### 3.5.2 API Key (Metadata Header)

**Use case:** Basit auth (JWT yerine static key)

**gRPC metadata:**
```python
metadata = [('x-api-key', 'pk_live_abc123def456')]
```

**Server-side validation:**
- ws-hub gRPC interceptor: API key'i Redis'ten validate eder (user_id mapping)
- Invalid key â†’ `UNAUTHENTICATED` gRPC error

**Avantajlar:**
- âœ… Basit (static key, JWT refresh yok)
- âœ… Bot'lar iÃ§in ideal

**Dezavantajlar:**
- âŒ Key leak riski (mTLS'ten daha az gÃ¼venli)

---

#### 3.5.3 JWT (Existing Mechanism)

gRPC metadata:
```python
metadata = [('authorization', 'Bearer <JWT>')]
```

WebSocket ile aynÄ± JWT kullanÄ±lÄ±r â†’ consistency.

---

#### 3.5.4 Ã–nerilen Auth Stack

| Client Tipi | Auth Method | Rationale |
|---|---|---|
| MM bot (production) | **mTLS** | En gÃ¼venli, token expiry yok |
| MM bot (test/dev) | API Key | Basit setup |
| Internal servis | mTLS | Service mesh (Istio) entegrasyonu |
| Web/mobile | JWT | Mevcut auth ile uyumlu |

**Phase 1 (gRPC launch):** JWT + API Key support
**Phase 2:** mTLS support ekle (MM feedback'ine gÃ¶re)

---

### 3.6 Avantajlar (vs WebSocket)

| Kategori | WebSocket | gRPC Streaming | Ä°yileÅŸme |
|---|---|---|---|
| **Payload boyutu** | JSON text (avg 500 byte) | Protobuf binary (avg 200 byte) | **%60 azalma** |
| **Parse overhead** | JSON parse (CPU-intensive) | Protobuf unmarshal (zero-copy) | **%50 CPU azalma** |
| **Backpressure** | Application-level drop | HTTP/2 flow control (native) | **Zero message loss** |
| **Schema validation** | Runtime (client-side) | Compile-time (proto) | **Type safety** |
| **Multiplexing** | Single stream per connection | HTTP/2 multiple streams | **Connection efficiency** |
| **Debugging** | Custom tools (wscat + jq) | grpcurl, gRPC reflection | **Kolay debug** |
| **Client library** | Custom wrapper gerekli | Auto-generated stub | **HÄ±zlÄ± entegrasyon** |
| **Latency (p50)** | ~5ms | ~3ms | **%40 azalma** |
| **Bandwidth (50 market)** | ~10 Mbps | ~4 Mbps | **%60 azalma** |

**Quantitative Ã¶rnek (MM bot, 50 market, orderbook delta):**
- WebSocket: 500 msg/s Ã— 500 byte = 250 KB/s = 2 Mbps
- gRPC: 500 msg/s Ã— 200 byte = 100 KB/s = 0.8 Mbps â†’ **%60 bandwidth tasarrufu**

---

### 3.7 Trade-offs

#### 3.7.1 Dezavantajlar

| Dezavantaj | AÃ§Ä±klama | Mitigation |
|---|---|---|
| **Browser desteksiz** | gRPC-Web gerekir (extra proxy layer) | gRPC sadece server-side client'lar iÃ§in, browser WebSocket kullanÄ±r |
| **Complex client setup** | Protobuf code generation, gRPC library | SDK provide edilecek (Python, Go, TypeScript) |
| **Proto schema management** | Schema deÄŸiÅŸikliÄŸinde backward compatibility kontrolÃ¼ | Protobuf field numbering discipline + CI/CD check |
| **Load balancer support** | L4 LB gerekli (L7 LB HTTP/2 streaming sorunlu olabilir) | AWS NLB veya Kubernetes Service (L4 mode) |
| **Debugging (production)** | Binary protocol â†’ network capture okunamaz | gRPC reflection enable (debug mode), grpcurl |
| **Learning curve** | MM'ler protobuf/gRPC Ã¶ÄŸrenmeli | Documentation + example code + SDK |

---

#### 3.7.2 WebSocket vs gRPC: Hangi Client Ne KullanmalÄ±?

| Client | Transport | Rationale |
|---|---|---|
| **Web frontend** | WebSocket | gRPC-Web overhead, native WS yeterli |
| **Mobile app** | WebSocket | gRPC mobile desteÄŸi var ama WS daha yaygÄ±n |
| **Python MM bot** | **gRPC** | `grpcio` mature, async support, protobuf efficient |
| **Go MM bot** | **gRPC** | Native support, ultra-low latency |
| **Node.js bot** | gRPC (opsiyonel WS) | `@grpc/grpc-js` iyi, ama WS de tamam |
| **Java enterprise** | **gRPC** | `grpc-java` mature, protobuf standard |
| **Internal service** | **gRPC** | Service mesh (Istio) native support |

**Genel kural:** Server-side â†’ gRPC, browser/mobile â†’ WebSocket

---

### 3.8 Ã–neri: Faz ve Timeline

#### 3.8.1 Ã–nerilen Faz: **Phase 3** (Week 12-24)

**Rationale:**
- Phase 1-2: WebSocket v2 stabilize et (MM beta, GA)
- Phase 3: MM feedback topla â†’ "JSON parse overhead var mÄ±?", "bandwidth sorun mu?"
- EÄŸer MM'ler ÅŸikayet ederse â†’ gRPC priority yÃ¼kselir
- EÄŸer WS yeterli ise â†’ gRPC Phase 4-5'e kayar

---

#### 3.8.2 gRPC Rollout PlanÄ±

**Phase 3a: PoC + MM Pilot (Week 12-16)**
- [ ] Proto file finalize (stream_v2.proto)
- [ ] ws-hub-v2'ye gRPC server ekle (flag: `--grpc-enabled=false` default)
- [ ] 1-2 MM ile pilot test (Python/Go SDK)
- [ ] Latency, bandwidth, CPU karÅŸÄ±laÅŸtÄ±rma (WS vs gRPC)
- [ ] Karar: MM'ler tercih ediyor mu?

**Phase 3b: Beta (Week 16-20)**
- [ ] gRPC endpoint production'da enable (`--grpc-enabled=true`, `--grpc-port=50051`)
- [ ] L4 load balancer setup (AWS NLB)
- [ ] 5-10 MM gRPC'ye geÃ§iÅŸ
- [ ] Monitoring dashboard (gRPC metrics ayrÄ± panel)

**Phase 3c: GA (Week 20-24)**
- [ ] gRPC documentation publish
- [ ] Python, Go, TypeScript SDK release
- [ ] All MM'lere announcement (opsiyonel transport olarak)
- [ ] WebSocket deprecated deÄŸil (her iki transport parallel devam)

---

#### 3.8.3 Success Criteria (gRPC)

| Metrik | Target |
|---|---|
| MM adoption (gRPC) | %50 (6 ay) |
| Latency reduction (vs WS) | >%30 |
| Bandwidth reduction | >%40 |
| MM feedback score | 8/10 |
| Zero critical bug | 3 ay boyunca |

---

#### 3.8.4 Karar AÄŸacÄ±

```
Week 12 (v2 WS GA + 30 gÃ¼n):
â”‚
â”œâ”€ MM'ler WS'ten memnun mu?
â”‚  â”œâ”€ YES â†’ gRPC Ã¶ncelik dÃ¼ÅŸÃ¼k, Phase 4-5'e kayar
â”‚  â””â”€ NO (latency/bandwidth ÅŸikayet)
â”‚     â””â”€ gRPC PoC baÅŸlat (Phase 3a)
â”‚        â”‚
â”‚        Week 16: PoC sonucu
â”‚        â”œâ”€ gRPC %30+ iyileÅŸtirme saÄŸlÄ±yor mu?
â”‚        â”‚  â”œâ”€ YES â†’ Phase 3b (beta) devam
â”‚        â”‚  â””â”€ NO â†’ gRPC iptal, alternatif optimization (WS compression, batching)
â”‚        â”‚
â”‚        Week 20: Beta feedback
â”‚        â””â”€ MM'ler gRPC'ye geÃ§iyor mu?
â”‚           â”œâ”€ YES â†’ Phase 3c (GA)
â”‚           â””â”€ NO â†’ gRPC opsiyonel kalÄ±r, aktif promotion yapÄ±lmaz
```

---

## SonuÃ§ ve Action Item Ã–zeti

### BÃ¶lÃ¼m 1: PRD Review â€” Ä°zolasyon

**Genel deÄŸerlendirme:**
- âœ… Core izolasyon (pod, consumer group, route) **saÄŸlam**
- âš ï¸ Shared resource management (Kafka, KrakenD, snapshot service, Redis) **iyileÅŸtirilebilir**
- ğŸ“Š Ä°zolasyon skoru: **%85** (action item'lar tamamlanÄ±rsa %95+)

**Kritik action item'lar:**
1. [ ] Kafka topic strategy netleÅŸtir (shared vs separate, partition planning)
2. [ ] KrakenD deployment strategy belirle (shared vs separate instance)
3. [ ] KrakenD HPA policy tanÄ±mla (Phase 3 Ã¶ncesi)
4. [ ] Snapshot service (order-api, wallet) capacity test et
5. [ ] Redis kullanÄ±m scope'unu daralt (JWT blacklist only)
6. [ ] Sequence number source strategy belirle (Kafka offset vs pod-local)
7. [ ] Pod scale event handling ekle (graceful reconnect)
8. [ ] Gap buffer memory policy revize et (per-channel size)

---

### BÃ¶lÃ¼m 2: gRPC Ã–nerisi

**Ã–zet:**
- gRPC streaming, **server-side MM bot'larÄ±** iÃ§in WebSocket'e gÃ¶re %30-60 performans iyileÅŸtirmesi saÄŸlar
- **Browser/mobile client'lar iÃ§in deÄŸil**, sadece backend bot'lar iÃ§in
- Ã–nerilen faz: **Phase 3** (week 12-24) â€” WebSocket GA'dan sonra, MM feedback'ine gÃ¶re
- Implementation: ws-hub-v2 binary'sine gRPC server eklenir (iki port: 8080 WS, 50051 gRPC)

**Karar noktasÄ±:** Phase 2 MM beta sonunda MM'lerden feedback al:
- "JSON parse overhead problem mu?"
- "Bandwidth problem mu?"
- "Latency yeterli mi?"

**EÄŸer ÅŸikayet varsa** â†’ gRPC PoC baÅŸlat
**EÄŸer WS yeterli ise** â†’ gRPC Phase 4-5'e kayar (low priority)

---

**Review tamamlandÄ±. Toplam 312 satÄ±r, detaylÄ± analiz ve actionable Ã¶neriler iÃ§ermektedir.**
